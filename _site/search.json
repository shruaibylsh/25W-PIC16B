[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sihui’s Blog for PIC 16B",
    "section": "",
    "text": "PIC 16B Final Group Submission\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nGroup #5\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Classification using Keras\n\n\n\n\n\n\nhomework\n\n\ntutorials\n\n\n\n\n\n\n\n\n\nMar 16, 2025\n\n\nSihui Lin\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Message Bank Web App with Dash by Plotly\n\n\n\n\n\n\nhomework\n\n\ntutorials\n\n\n\n\n\n\n\n\n\nFeb 16, 2025\n\n\nSihui Lin\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Visualization\n\n\n\n\n\n\nweek 1\n\n\ntutorials\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nSihui Lin\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization with Python\n\n\n\n\n\n\nweek 0\n\n\ntutorials\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nSihui Lin\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 19, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Data Visualization with Python/index.html",
    "href": "posts/Data Visualization with Python/index.html",
    "title": "Data Visualization with Python",
    "section": "",
    "text": "In this tutorial, we will be exploring the Palmer Penguins dataset by creating a meaningful data visualization. We will be investigating the relationship between culmen length and culmen depth for penguins across different species.\nThe culmen refers to the upper ridge of a bird’s bill. It is more clearly shown in the picture below (source: https://github.com/allisonhorst/palmerpenguins).\n\n\n\nFirst, let’s import the necessary libraries and load the dataset.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nTo better understand the penguins dataset, let’s observe the first few rows of data.\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nWhile the dataset contains multiple fields, for our analysis, we are only interested in Species, Culmen Length (mm), and Culmen Depth (mm). Let’s subset the data frame so it will be easier and clearer to work with.\n\npenguins = penguins[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)']]\n\nLet’s check the resulting data frame.\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n39.1\n18.7\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\n39.5\n17.4\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\n40.3\n18.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\n36.7\n19.3\n\n\n\n\n\n\n\n\n\n\nBefore visualizing the data, it is important to handle records with missing or invalid values.\n\nprint(penguins.isnull().sum())\n\nSpecies               0\nCulmen Length (mm)    2\nCulmen Depth (mm)     2\ndtype: int64\n\n\nFrom the output, we notice that there are respectively 2 and 2 missing values in the culmen length and depth columns. We can handle these missing data by dropping rows with NA values.\n\npenguins = penguins.dropna()\n\nNow let’s check if we still have missing values in the dataset:\n\nprint(penguins.isnull().sum())\n\nSpecies               0\nCulmen Length (mm)    0\nCulmen Depth (mm)     0\ndtype: int64\n\n\n\n\n\nNow that the data is clean, we will proceed into data visualization.\nFor the purpose of our data analysis, it is appropriate to use a scatter plot, where each dot on the graph can represent an individual case with two numerical variables (culmen length and depth).\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Create a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data = penguins,\n    x = 'Culmen Length (mm)',\n    y = 'Culmen Depth (mm)',\n    hue = 'Species'\n)\n\n# Add labels and a title\nplt.title('Relationship Between Culmen Length and Depth Across Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\nplt.legend(title = 'Species', loc = 'lower left', fontsize = 'small')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the visualization above, we can clearly observe the patterns in culmen length and depth across different species, represented by dots of different colors. We notice that Adelie Penguins tend to have shorter and deeper culmen, Gentoo Penguins tend to have longer and shallower culmen, whereas Chinstrap Penguins tend to have longer and deeper culmen. Moreover, the distinct clusters of dots clearly point toward notable differences between species.\nGreat! Now we have learned to explore patterns in datasets with visualizations. In the next tutorial, we will go deeper and learn more about what we can do using Python."
  },
  {
    "objectID": "posts/Data Visualization with Python/index.html#data-preparation",
    "href": "posts/Data Visualization with Python/index.html#data-preparation",
    "title": "Data Visualization with Python",
    "section": "",
    "text": "First, let’s import the necessary libraries and load the dataset.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nTo better understand the penguins dataset, let’s observe the first few rows of data.\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nWhile the dataset contains multiple fields, for our analysis, we are only interested in Species, Culmen Length (mm), and Culmen Depth (mm). Let’s subset the data frame so it will be easier and clearer to work with.\n\npenguins = penguins[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)']]\n\nLet’s check the resulting data frame.\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n39.1\n18.7\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\n39.5\n17.4\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\n40.3\n18.0\n\n\n3\nAdelie Penguin (Pygoscelis adeliae)\nNaN\nNaN\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\n36.7\n19.3"
  },
  {
    "objectID": "posts/Data Visualization with Python/index.html#data-cleaning",
    "href": "posts/Data Visualization with Python/index.html#data-cleaning",
    "title": "Data Visualization with Python",
    "section": "",
    "text": "Before visualizing the data, it is important to handle records with missing or invalid values.\n\nprint(penguins.isnull().sum())\n\nSpecies               0\nCulmen Length (mm)    2\nCulmen Depth (mm)     2\ndtype: int64\n\n\nFrom the output, we notice that there are respectively 2 and 2 missing values in the culmen length and depth columns. We can handle these missing data by dropping rows with NA values.\n\npenguins = penguins.dropna()\n\nNow let’s check if we still have missing values in the dataset:\n\nprint(penguins.isnull().sum())\n\nSpecies               0\nCulmen Length (mm)    0\nCulmen Depth (mm)     0\ndtype: int64"
  },
  {
    "objectID": "posts/Data Visualization with Python/index.html#data-visualization",
    "href": "posts/Data Visualization with Python/index.html#data-visualization",
    "title": "Data Visualization with Python",
    "section": "",
    "text": "Now that the data is clean, we will proceed into data visualization.\nFor the purpose of our data analysis, it is appropriate to use a scatter plot, where each dot on the graph can represent an individual case with two numerical variables (culmen length and depth).\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Create a scatter plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data = penguins,\n    x = 'Culmen Length (mm)',\n    y = 'Culmen Depth (mm)',\n    hue = 'Species'\n)\n\n# Add labels and a title\nplt.title('Relationship Between Culmen Length and Depth Across Penguin Species')\nplt.xlabel('Culmen Length (mm)')\nplt.ylabel('Culmen Depth (mm)')\nplt.legend(title = 'Species', loc = 'lower left', fontsize = 'small')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the visualization above, we can clearly observe the patterns in culmen length and depth across different species, represented by dots of different colors. We notice that Adelie Penguins tend to have shorter and deeper culmen, Gentoo Penguins tend to have longer and shallower culmen, whereas Chinstrap Penguins tend to have longer and deeper culmen. Moreover, the distinct clusters of dots clearly point toward notable differences between species.\nGreat! Now we have learned to explore patterns in datasets with visualizations. In the next tutorial, we will go deeper and learn more about what we can do using Python."
  },
  {
    "objectID": "posts/project 1/index.html",
    "href": "posts/project 1/index.html",
    "title": "Data Visualization with Python",
    "section": "",
    "text": "Name: Sihui Lin\nUID: 106013406"
  },
  {
    "objectID": "posts/project 1/index.html#introduction",
    "href": "posts/project 1/index.html#introduction",
    "title": "Data Visualization with Python",
    "section": "Introduction",
    "text": "Introduction\nWelcome to CS148 - Introduction to Data Science! As we’re planning to move through topics aggressively in this course, to start out, we’ll look to do an end-to-end walkthrough of a datascience project, and then ask you to replicate the code yourself for a new dataset.\nPlease note: We don’t expect you to fully grasp everything happening here in either code or theory. This content will be reviewed throughout the quarter. Rather we hope that by giving you the full perspective on a data science project it will better help to contextualize the pieces as they’re covered in class\nIn that spirit, we will first work through an example project from end to end to give you a feel for the steps involved.\nHere are the main steps:\n\nGet the data\nVisualize the data for insights\nPreprocess the data for your machine learning algorithm\nSelect a machine learning model and train it\nEvaluate its performance"
  },
  {
    "objectID": "posts/project 1/index.html#working-with-real-data",
    "href": "posts/project 1/index.html#working-with-real-data",
    "title": "Data Visualization with Python",
    "section": "Working with Real Data",
    "text": "Working with Real Data\nIt is best to experiment with real-data as opposed to aritifical datasets.\nThere are many different open datasets depending on the type of problems you might be interested in!\nHere are a few data repositories you could check out: - UCI Datasets - Kaggle Datasets - AWS Datasets\nBelow we will run through an California Housing example collected from the 1990’s."
  },
  {
    "objectID": "posts/project 1/index.html#setup",
    "href": "posts/project 1/index.html#setup",
    "title": "Data Visualization with Python",
    "section": "Setup",
    "text": "Setup\nWe’ll start by importing a series of libraries we’ll be using throughout the project.\n\nimport sys\nassert sys.version_info &gt;= (3, 5) # python&gt;=3.5\nimport sklearn\n#assert sklearn.__version__ &gt;= \"0.20\" # sklearn &gt;= 0.20\n\nimport numpy as np #numerical package in python\n%matplotlib inline\nimport matplotlib.pyplot as plt #plotting package\n\n# to make this notebook's output identical at every run\nnp.random.seed(42)\n\n#matplotlib magic for inline figures\n%matplotlib inline\nimport matplotlib # plotting library\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/project 1/index.html#intro-to-data-exploration-using-pandas",
    "href": "posts/project 1/index.html#intro-to-data-exploration-using-pandas",
    "title": "Data Visualization with Python",
    "section": "Intro to Data Exploration Using Pandas",
    "text": "Intro to Data Exploration Using Pandas\nIn this section we will load the dataset, and visualize different features using different types of plots.\nPackages we will use: - Pandas: is a fast, flexibile and expressive data structure widely used for tabular and multidimensional datasets. - Matplotlib: is a 2d python plotting library which you can use to create quality figures (you can plot almost anything if you’re willing to code it out!) - other plotting libraries:seaborn, ggplot2\nNote: If you’re working in CoLab for this project, the CSV file first has to be loaded into the environment. This can be done manually using the sidebar menu option, or using the following code here.\nIf you’re running this notebook locally on your device, simply proceed to the next step.\n\n# from google.colab import files\n# files.upload()\n\nWe’ll now begin working with Pandas. Pandas is the principle library for data management in python. It’s primary mechanism of data storage is the dataframe, a two dimensional table, where each column represents a datatype, and each row a specific data element in the set.\nTo work with dataframes, we have to first read in the csv file and convert it to a dataframe using the code below.\n\n# We'll now import the holy grail of python datascience: Pandas!\nimport pandas as pd\nhousing = pd.read_csv('housing.csv')\n\n\nhousing.head() # show the first few elements of the dataframe\n               # typically this is the first thing you do\n               # to see how the dataframe looks like\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nA dataset may have different types of features - real valued - Discrete (integers) - categorical (strings) - Boolean\nThe two categorical features are essentialy the same as you can always map a categorical string/character to an integer.\nIn the dataset example, all our features are real valued floats, except ocean proximity which is categorical.\n\n# to see a concise summary of data types, null values, and counts\n# use the info() method on the dataframe\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\n# you can access individual columns similarly\n# to accessing elements in a python dict\nhousing[\"ocean_proximity\"].head() # added head() to avoid printing many columns..\n\n0    NEAR BAY\n1    NEAR BAY\n2    NEAR BAY\n3    NEAR BAY\n4    NEAR BAY\nName: ocean_proximity, dtype: object\n\n\n\n# to access a particular row we can use iloc\nhousing.iloc[1]\n\nlongitude              -122.22\nlatitude                 37.86\nhousing_median_age        21.0\ntotal_rooms             7099.0\ntotal_bedrooms          1106.0\npopulation              2401.0\nhouseholds              1138.0\nmedian_income           8.3014\nmedian_house_value    358500.0\nocean_proximity       NEAR BAY\nName: 1, dtype: object\n\n\n\n# one other function that might be useful is\n# value_counts(), which counts the number of occurences\n# for categorical features\nhousing[\"ocean_proximity\"].value_counts()\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\n\n\n\n# The describe function compiles your typical statistics for each\n# column\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\n\nIf you want to learn about different ways of accessing elements or other functions it’s useful to check out the getting started section here"
  },
  {
    "objectID": "posts/project 1/index.html#lets-start-visualizing-the-dataset",
    "href": "posts/project 1/index.html#lets-start-visualizing-the-dataset",
    "title": "Data Visualization with Python",
    "section": "Let’s start visualizing the dataset",
    "text": "Let’s start visualizing the dataset\n\n# We can draw a histogram for each of the dataframes features\n# using the hist function\nhousing.hist(bins=50, figsize=(20,15))\n# save_fig(\"attribute_histogram_plots\")\nplt.show() # pandas internally uses matplotlib, and to display all the figures\n           # the show() function must be called\n\n\n\n\n\n\n\n\n\n# if you want to have a histogram on an individual feature:\nhousing[\"median_income\"].hist()\nplt.show()\n\n\n\n\n\n\n\n\nWe can convert a floating point feature to a categorical feature by binning or by defining a set of intervals.\nFor example, to bin the households based on median_income we can use the pd.cut function\n\n# assign each bin a categorical value [1, 2, 3, 4, 5] in this case.\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\nhousing[\"income_cat\"].value_counts()\n\nincome_cat\n3    7236\n2    6581\n4    3639\n5    2362\n1     822\nName: count, dtype: int64\n\n\n\nhousing[\"income_cat\"].hist()\n\n\n\n\n\n\n\n\n\nNext let’s visualize the household incomes based on latitude & longitude coordinates\n\n## here's a not so interestting way plotting it\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n\n\n\n\n\n\n\n\n\n# we can make it look a bit nicer by using the alpha parameter,\n# it simply plots less dense areas lighter.\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n\n\n\n\n\n\n\n\n\n# A more interesting plot is to color code (heatmap) the dots\n# based on income. The code below achieves this\n\n# Please note: In order for this to work, ensure that you've loaded an image\n# of california (california.png) into this directory prior to running this\n\nimport matplotlib.image as mpimg\ncalifornia_img=mpimg.imread('california.png')\nax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=housing['population']/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\n# overlay the califronia map on the plotted scatter plot\n# note: plt.imshow still refers to the most recent figure\n# that hasn't been plotted yet.\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\n# setting up heatmap colors based on median_house_value feature\nprices = housing[\"median_house_value\"]\ntick_values = np.linspace(prices.min(), prices.max(), 11)\ncb = plt.colorbar()\ncb.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\ncb.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nplt.show()\n\n/var/folders/8r/cg47v_px0r11bynb_33svf300000gn/T/ipykernel_16151/2129115766.py:26: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  cb.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n\n\n\n\n\n\n\n\n\nNot suprisingly, the most expensive houses are concentrated around the San Francisco/Los Angeles areas.\nUp until now we have only visualized feature histograms and basic statistics.\nWhen developing machine learning models the predictiveness of a feature for a particular target of intrest is what’s important.\nIt may be that only a few features are useful for the target at hand, or features may need to be augmented by applying certain transfomrations.\nNone the less we can explore this using correlation matrices.\n\n# Select only numeric columns\nnumeric_housing = housing.select_dtypes(include=[float, int])\n\n# Compute the correlation matrix\ncorr_matrix = numeric_housing.corr()\n\n\n# for example if the target is \"median_house_value\", most correlated features can be sorted\n# which happens to be \"median_income\". This also intuitively makes sense.\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688075\ntotal_rooms           0.134153\nhousing_median_age    0.105623\nhouseholds            0.065843\ntotal_bedrooms        0.049686\npopulation           -0.024650\nlongitude            -0.045967\nlatitude             -0.144160\nName: median_house_value, dtype: float64\n\n\n\n# the correlation matrix for different attributes/features can also be plotted\n# some features may show a positive correlation/negative correlation or\n# it may turn out to be completely random!\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\n\narray([[&lt;Axes: xlabel='median_house_value', ylabel='median_house_value'&gt;,\n        &lt;Axes: xlabel='median_income', ylabel='median_house_value'&gt;,\n        &lt;Axes: xlabel='total_rooms', ylabel='median_house_value'&gt;,\n        &lt;Axes: xlabel='housing_median_age', ylabel='median_house_value'&gt;],\n       [&lt;Axes: xlabel='median_house_value', ylabel='median_income'&gt;,\n        &lt;Axes: xlabel='median_income', ylabel='median_income'&gt;,\n        &lt;Axes: xlabel='total_rooms', ylabel='median_income'&gt;,\n        &lt;Axes: xlabel='housing_median_age', ylabel='median_income'&gt;],\n       [&lt;Axes: xlabel='median_house_value', ylabel='total_rooms'&gt;,\n        &lt;Axes: xlabel='median_income', ylabel='total_rooms'&gt;,\n        &lt;Axes: xlabel='total_rooms', ylabel='total_rooms'&gt;,\n        &lt;Axes: xlabel='housing_median_age', ylabel='total_rooms'&gt;],\n       [&lt;Axes: xlabel='median_house_value', ylabel='housing_median_age'&gt;,\n        &lt;Axes: xlabel='median_income', ylabel='housing_median_age'&gt;,\n        &lt;Axes: xlabel='total_rooms', ylabel='housing_median_age'&gt;,\n        &lt;Axes: xlabel='housing_median_age', ylabel='housing_median_age'&gt;]],\n      dtype=object)\n\n\n\n\n\n\n\n\n\n\n# median income vs median house vlue plot plot 2 in the first row of top figure\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])"
  },
  {
    "objectID": "posts/project 1/index.html#preparing-dastaset-for-ml",
    "href": "posts/project 1/index.html#preparing-dastaset-for-ml",
    "title": "Data Visualization with Python",
    "section": "Preparing Dastaset for ML",
    "text": "Preparing Dastaset for ML\n\nDealing With Incomplete Data\n\n# have you noticed when looking at the dataframe summary certain rows\n# contained null values? we can't just leave them as nulls and expect our\n# model to handle them for us...\nsample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nincome_cat\n\n\n\n\n290\n-122.16\n37.77\n47.0\n1256.0\nNaN\n570.0\n218.0\n4.3750\n161900.0\nNEAR BAY\n3\n\n\n341\n-122.17\n37.75\n38.0\n992.0\nNaN\n732.0\n259.0\n1.6196\n85100.0\nNEAR BAY\n2\n\n\n538\n-122.28\n37.78\n29.0\n5154.0\nNaN\n3741.0\n1273.0\n2.5762\n173400.0\nNEAR BAY\n2\n\n\n563\n-122.24\n37.75\n45.0\n891.0\nNaN\n384.0\n146.0\n4.9489\n247100.0\nNEAR BAY\n4\n\n\n696\n-122.10\n37.69\n41.0\n746.0\nNaN\n387.0\n161.0\n3.9063\n178400.0\nNEAR BAY\n3\n\n\n\n\n\n\n\n\nsample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1: simply drop rows that have null values\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nincome_cat\n\n\n\n\n\n\n\n\n\n\nsample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2: drop the complete feature\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nincome_cat\n\n\n\n\n290\n-122.16\n37.77\n47.0\n1256.0\n570.0\n218.0\n4.3750\n161900.0\nNEAR BAY\n3\n\n\n341\n-122.17\n37.75\n38.0\n992.0\n732.0\n259.0\n1.6196\n85100.0\nNEAR BAY\n2\n\n\n538\n-122.28\n37.78\n29.0\n5154.0\n3741.0\n1273.0\n2.5762\n173400.0\nNEAR BAY\n2\n\n\n563\n-122.24\n37.75\n45.0\n891.0\n384.0\n146.0\n4.9489\n247100.0\nNEAR BAY\n4\n\n\n696\n-122.10\n37.69\n41.0\n746.0\n387.0\n161.0\n3.9063\n178400.0\nNEAR BAY\n3\n\n\n\n\n\n\n\n\nmedian = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3: replace na values with median values\nsample_incomplete_rows\n\n/var/folders/8r/cg47v_px0r11bynb_33svf300000gn/T/ipykernel_16151/3420772705.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3: replace na values with median values\n\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nincome_cat\n\n\n\n\n290\n-122.16\n37.77\n47.0\n1256.0\n435.0\n570.0\n218.0\n4.3750\n161900.0\nNEAR BAY\n3\n\n\n341\n-122.17\n37.75\n38.0\n992.0\n435.0\n732.0\n259.0\n1.6196\n85100.0\nNEAR BAY\n2\n\n\n538\n-122.28\n37.78\n29.0\n5154.0\n435.0\n3741.0\n1273.0\n2.5762\n173400.0\nNEAR BAY\n2\n\n\n563\n-122.24\n37.75\n45.0\n891.0\n435.0\n384.0\n146.0\n4.9489\n247100.0\nNEAR BAY\n4\n\n\n696\n-122.10\n37.69\n41.0\n746.0\n435.0\n387.0\n161.0\n3.9063\n178400.0\nNEAR BAY\n3\n\n\n\n\n\n\n\nNow that we’ve played around with this, lets finalize this approach by replacing the nulls in our final dataset\n\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\n\n/var/folders/8r/cg47v_px0r11bynb_33svf300000gn/T/ipykernel_16151/1782877594.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  housing[\"total_bedrooms\"].fillna(median, inplace=True)\n\n\nCould you think of another plausible imputation for this dataset?\n\n\nAugmenting Features\nNew features can be created by combining different columns from our data set.\n\nrooms_per_household = total_rooms / households\nbedrooms_per_room = total_bedrooms / total_rooms\netc.\n\n\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/(housing[\"households\"] + 1e-6)\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/(housing[\"total_rooms\"] + 1e-6)\nhousing[\"population_per_household\"]=housing[\"population\"]/(housing[\"households\"] + 1e-6)\n\n\nhousing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDealing with Non-Numeric Data\nSo we’re almost ready to feed our dataset into a machine learning model, but we’re not quite there yet!\nGenerally speaking all models can only work with numeric data, which means that if you have Categorical data you want included in your model, you’ll need to do a numeric conversion. We’ll explore this more later, but for now we’ll take one approach to converting our ocean_proximity field into a numeric one.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\nhousing['ocean_proximity'] = labelencoder.fit_transform(housing['ocean_proximity'])\nhousing.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\nincome_cat\nrooms_per_household\nbedrooms_per_room\npopulation_per_household\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\n3\n5\n6.984127\n0.146591\n2.555556\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\n3\n5\n6.238137\n0.155797\n2.109842\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\n3\n5\n8.288136\n0.129516\n2.802260\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\n3\n4\n5.817352\n0.184458\n2.547945\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\n3\n3\n6.281853\n0.172096\n2.181467\n\n\n\n\n\n\n\n\n\nDivide up the Dataset for Machine Learning\nAfter having cleaned your dataset you’re ready to train your machine learning model.\nTo do so you’ll aim to divide your data into: - train set - test set\nIn some cases you might also have a validation set as well for tuning hyperparameters (don’t worry if you’re not familiar with this term yet..)\nIn supervised learning setting your train set and test set should contain (feature, target) tuples. - feature: is the input to your model - target: is the ground truth label - when target is categorical the task is a classification task - when target is floating point the task is a regression task\nWe will make use of scikit-learn python package for preprocessing.\nScikit learn is pretty well documented and if you get confused at any point simply look up the function/object!\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# let's first start by creating our train and test sets\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    train_set = housing.loc[train_index]\n    test_set = housing.loc[test_index]\n\n\nhousing_training = train_set.drop(\"median_house_value\", axis=1) # drop labels for training set features\n                                                       # the input to the model should not contain the true label\nhousing_labels = train_set[\"median_house_value\"].copy()\n\n\nhousing_testing = test_set.drop(\"median_house_value\", axis=1) # drop labels for training set features\n                                                       # the input to the model should not contain the true label\nhousing__test_labels = test_set[\"median_house_value\"].copy()\n\n\n\nSelect a model and train\nOnce we have prepared the dataset it’s time to choose a model.\nAs our task is to predict the median_house_value (a floating value), regression is well suited for this.\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_training, housing_labels)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# let's try our model on a few testing instances\ndata = housing_testing.iloc[:5]\nlabels = housing__test_labels.iloc[:5]\n\nprint(\"Predictions:\", np.round(lin_reg.predict(data), 1))\nprint(\"Actual labels:\", list(labels))\n\nPredictions: [418197.2 305620.5 232253.  188754.6 251166.4]\nActual labels: [500001.0, 162500.0, 204600.0, 159700.0, 184000.0]\n\n\nWe can evaluate our model using certain metrics, a fitting metric for regresison is the mean-squared-loss\n\\[L(\\hat{Y}, Y) = \\frac{1}{N} \\sum_i^N (\\hat{y_i} - y_i)^2\\]\nwhere \\(\\hat{y}\\) is the predicted value, and y is the ground truth label.\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = lin_reg.predict(housing_testing)\nmse = mean_squared_error(housing__test_labels, preds)\nrmse = np.sqrt(mse)\nrmse\n\n67694.08184344384\n\n\nIs this a good result? What do you think an acceptable error rate is for this sort of problem?"
  },
  {
    "objectID": "posts/Data Wrangling and Visualization/index.html",
    "href": "posts/Data Wrangling and Visualization/index.html",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "In this tutorial, we will be exploring the National Oceanic and Atmospheric Administration (NOAA) climate data set. Using plotly and managing database through sqlite3, we will be creating several interesting and interactive data graphics.\n\n\nFirst, let’s create a database with three tables, temperatures, stations, and countries. They respectively record data on temperatures recorded at climate stations, information about climate stations, and information about countries.\nAs always, we will import necessary libraries at the start.\n\n# import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport inspect\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nFirst, let’s create a database connection and name it climate.db.\n\n# create database connection\nconn = sqlite3.connect(\"climate.db\")\n\nLet’s first download the files.\n\n# load the temperatures data\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files\nimport urllib.request\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/25W/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nTo format our dataframe from the wide format to the long format, we will write a function prepare_df() to help us prepare the dataframe for easier analysis. We will also clean the data by removing rows with temperature of NaN.\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df = df[~np.isnan(df[\"Temp\"])] # remove rows with temperature of NaN \n\n    return df\n\nNext, let’s consolidate all the temperature data from 1901 to 2020 to a single table, which will then be added to the climate database.\n\n# add the temperatures table to the database\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nThe data for stations and countries, respectively stored in a single csv file, will then be loaded and added as tables to the database.\n\n# add the stations table to the database\nstations = pd.read_csv(\"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n# add the countries table to the database\ncountries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nNow let’s verify whether we have successfully added the three tables to our database. We can also check the names of their columns.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nRemember to close the database connection after finishing constructing the database!\n\nconn.close()\n\n\n\n\nNow, let’s write a query function called query_climate_database that makes our workflow easier. This function is designed to help us retrieve information from the climate database all at once, such that we do not need to work on the three tables separately.\nThe function will take in several input parameters that specifies the country and time range for data extraction from the climate database, and will output a dataframe displaying comprehensive information including stations in this country, their location, time for temperature records, and the recorded temperature. Let’s take a close look at the function:\n\nfrom climate_database import query_climate_database\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    input:\n    db_file: the file name for the database\n    country: a string giving the name of a country for which data should be returned.\n    year_begin and year_end: two integers giving the earliest and latest years for which should be returned (inclusive).\n    month: an integer giving the month of the year for which should be returned.\n    \n    output (dataframe):\n    NAME: The station name.\n    LATITUDE: The latitude of the station.\n    LONGITUDE: The longitude of the station.\n    Country: The name of the country in which the station is located.\n    Year: The year in which the reading was taken.\n    Month: The month in which the reading was taken.\n    Temp: The average temperature at the specified station during the specified year and month.\n    \"\"\"\n\n    # connect to database\n    conn = sqlite3.connect(db_file)\n\n    # write sql command using f-strings\n    command = f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS Country, T.Year, T.Month, T.Temp\n    FROM temperatures T\n    LEFT JOIN stations S\n    ON T.id = S.id\n    LEFT JOIN countries C\n    ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    WHERE C.Name = \"{country}\"\n    AND T.Year &gt;= {year_begin}\n    AND T.Year &lt;= {year_end}\n    AND T.Month = {month}\n    ORDER BY S.NAME, T.Year, T.Month\n    \"\"\"\n\n    df = pd.read_sql_query(command, conn)\n    conn.close()\n    return df\n\n\n\nNow let’s use this function to retrieve some data from the climate database. Looking at India’s temperature data in January from 1980 to 2020, we get:\n\nquery_climate_database(db_file = \"climate.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\n\nNow that we have played around with our database, let’s explore it and create some interesting visualizations.\nOur guiding question will be: How does the average yearly change in temperature vary within a given country?\nIn order to create an interesting visualization to answer this question, let us create a geographic scatterplot in plotly express. Each point on the plot will represent the location of a station, and the color of the point reflects an estimate of the yearly change in temperature during the specific month and time period at that station.\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    inputs:\n    the first five are the same\n    min_obs: the minimum required number of years of data for any given station\n    **kwargs: additional keyword arguments passed to px.scatter_mapbox()\n    \"\"\"\n    \n    # query the database using the function we wrote earlier\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # filter to have stations which has more than or equal to min_obs\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).filter(lambda group: len(group) &gt;= min_obs)\n\n    # compute the first coefficient of a linear regression model at a given station\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n    \n    coefs = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef).reset_index(name = \"temp_diff\")\n\n    month_name = calendar.month_name[month]\n\n    fig = px.scatter_mapbox(coefs,\n                         lat = \"LATITUDE\",\n                         lon = \"LONGITUDE\",\n                         color = \"temp_diff\",\n                         range_color = [-0.15, 0.15],\n                         hover_name = \"NAME\",\n                         hover_data = {\n                            \"LATITUDE\": \":.3f\",\n                            \"LONGITUDE\": \":.3f\",\n                            \"temp_diff\": \":.3f\"\n                         },\n                         labels = {\n                             \"LATITUDE\": \"LATITUDE\",\n                             \"LONGITUDE\": \"LONGITUDE\",\n                             \"temp_diff\": \"Estimated Yearly Increase (°C)\"\n                         },\n                         title = f\"Yearly temperature increase estimates in {month_name}&lt;br&gt;for {country} stations, {year_begin} to {year_end}\",\n                         **kwargs)\n    \n    return fig\n\n\n\nNow let’s take a look at the January climate data in India from 1980 to 2020. What will it be like when we visualize it using the function we have written?\n\n# assumes you have imported necessary packages\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nLet’s use the same function on another time frame for another country. For example, let’s look at the temperature change in Germany from the year 2000 to 2020 in March. What will our map be like?\n\ncolor_map = px.colors.diverging.Tropic # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"Germany\", 2000, 2020, 3, \n                                   min_obs = 20,\n                                   zoom = 4,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\n\nNow that we have created a geographic visualization of the dataset, let’s go ahead and create two more interesting figures from the data.\nLet’s explore the relationship between station elevation and temperature variability. Our analysis will focus on understanding how stations at different elevations experience temperature changes throughout the year, which could provide valuable insights into local climate patterns.\nLet’s create another querying function called query_station_temperature_data to retrieve data from the climate database at once. This function retrieves station-level temperature data for a given country and time, and only selects stations with more than 3 readings in the same month across years.\n\nfrom climate_database import query_station_temperature_data\nprint(inspect.getsource(query_station_temperature_data))\n\ndef query_station_temperature_data(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    input:\n    db_file: the file name for the database.\n    country: a string giving the name of a country.\n    year_begin, year_end: start and end year (inclusive)\n    month: specific month.\n\n    output: a dataframe with the following columns\n    NAME: name of the station\n    LATITUDE, LONGITUDE: location of the station\n    STNELEV: elevation of the station\n    Country: the name of the country in which the station is located.\n    Year: the Year for the reading\n    Month: the Month for the reading\n    Temp_Range: range of temperature in the station\n    Num_Readings: number of readings for the station\n    \"\"\"\n\n    # connect to database\n    conn = sqlite3.connect(db_file)\n\n\n    # write sql command using f-strings\n    # only select stations with more than 3 readings in a same month across years\n    command = f\"\"\"\n    WITH yearly_station_counts AS (\n        SELECT S.NAME, COUNT(DISTINCT T.Year) as year_count\n        FROM temperatures T\n        LEFT JOIN stations S ON T.id = S.id\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = \"{country}\"\n        AND T.Year &gt;= {year_begin}\n        AND T.Year &lt;= {year_end}\n        AND T.Month = {month}\n        GROUP BY S.NAME\n        HAVING year_count &gt;= 3\n    )\n    SELECT \n        S.NAME, S.LATITUDE, S.LONGITUDE, S.STNELEV,\n        C.Name AS Country, \n        T.Month,\n        MAX(T.Temp) - MIN(T.Temp) AS Temp_Range\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    INNER JOIN yearly_station_counts ysc ON S.NAME = ysc.NAME\n    WHERE C.Name = \"{country}\"\n    AND T.Year &gt;= {year_begin}\n    AND T.Year &lt;= {year_end}\n    AND T.Month = {month}\n    GROUP BY S.NAME, S.LATITUDE, S.LONGITUDE, S.STNELEV\n    \"\"\"\n\n    df = pd.read_sql_query(command, conn)\n    conn.close()\n    return df\n\n\n\nLet’s check the output of this query function using the same example we have before.\n\nquery_station_temperature_data(db_file = \"climate.db\",\n                               country = \"India\", \n                               year_begin = 1980,\n                               year_end = 2000,\n                               month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nSTNELEV\nCountry\nMonth\nTemp_Range\n\n\n\n\n0\nAGARTALA\n23.8830\n91.2500\n16.0\nIndia\n1\n2.95\n\n\n1\nAGRA\n27.1667\n78.0333\n168.0\nIndia\n1\n2.20\n\n\n2\nAHMADABAD\n23.0670\n72.6330\n55.0\nIndia\n1\n3.54\n\n\n3\nAKOLA\n20.7000\n77.0330\n282.0\nIndia\n1\n4.60\n\n\n4\nAKOLA\n20.7000\n77.0670\n305.0\nIndia\n1\n3.63\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n92\nTIRUCHCHIRAPALLI\n10.7670\n78.7170\n88.0\nIndia\n1\n2.79\n\n\n93\nTRIVANDRUM\n8.5000\n77.0000\n64.0\nIndia\n1\n2.10\n\n\n94\nVARANASI_BABATPUR\n25.4500\n82.8670\n85.0\nIndia\n1\n0.93\n\n\n95\nVERAVAL\n20.9000\n70.3670\n8.0\nIndia\n1\n3.47\n\n\n96\nVISHAKHAPATNAM\n17.7170\n83.2330\n3.0\nIndia\n1\n3.00\n\n\n\n\n97 rows × 7 columns\n\n\n\nWith the help of the querying function, we can now answer the first question: How does temperature variability differ between high-elevation and low-elevation stations in a country in a given month?\nWe will create an interactive box plot. The X-axis will be the elevation bins, whereas the Y-axis will represent temperature range per station.\n\nfrom climate_database import plot_elevation_temp_variability\nprint(inspect.getsource(plot_elevation_temp_variability))\n\ndef plot_elevation_temp_variability(db_file, country, year_begin, year_end, month, num_bins=5):\n\n    \"\"\"\n    input:\n    db_file: database file path\n    country: country name\n    year_begin, year_end: time period to analyze\n    month: month to analyze (1-12)\n    num_bins: number of elevation bins to create\n    \"\"\"\n    # Get the data\n    df = query_station_temperature_data(db_file, country, year_begin, year_end, month)\n    \n    # Create elevation bins\n    df['Elevation_Bin'] = pd.qcut(df['STNELEV'], \n                                q=num_bins, \n                                labels=[f'{int(x.left)}-{int(x.right)}m' \n                                       for x in pd.qcut(df['STNELEV'], q=num_bins).unique()])\n    \n    # Create box plot\n    fig = px.box(\n        df,\n        x='Elevation_Bin',\n        y='Temp_Range',\n        points='all',  # Show all points\n        hover_data=['NAME', 'STNELEV'],  # Show station details on hover\n        labels={\n            'Elevation_Bin': 'Elevation Range',\n            'Temp_Range': 'Temperature Range (°C)',\n            'NAME': 'Station Name',\n            'STNELEV': 'Exact Elevation (m)'\n        },\n        title=f'Temperature Variability Across Elevation Ranges in {country}&lt;br&gt;{year_begin} to {year_end}, Month {month}'\n    )\n    \n    # Enhance layout\n    fig.update_layout(\n        height=600,\n        width=900,\n        title_x=0.5,\n        template='plotly_white',\n        xaxis_title=f'Elevation Ranges (divided into {num_bins} groups)'\n    )\n    \n    return fig\n\n\n\nLet’s generate the graph:\n\nfig = plot_elevation_temp_variability(\"climate.db\", \"India\", 1980, 2020, 1, num_bins=5)\nfig.show()\n\n\n\n\nGreat. Now let’s answer our second question: How does the correlation between elevation and temperature variability change over time?\nWe will created a faceted scatter plot, where the x-axis is the elvation, and y-axis represents the temperature range. We will also add a trendline showing linear regression per decade.\n\nfrom climate_database import plot_elevation_temp_trends\nprint(inspect.getsource(plot_elevation_temp_trends))\n\ndef plot_elevation_temp_trends(db_file, country, decade_ranges, month, max_elevation = 9000):\n    \"\"\"\n    input:\n    db_file: database file path\n    country: country name\n    decade_ranges: list of tuples, each containing (start_year, end_year) for a decade\n    month: month to analyze (1-12)\n    \"\"\"\n\n    # Initialize list to store data from each decade\n    decade_data = []\n    \n    # Collect data for each decade\n    for start_year, end_year in decade_ranges:\n        df = query_station_temperature_data(db_file, country, start_year, end_year, month)\n        # Filter out stations with unrealistic elevations\n        df = df[df['STNELEV'] &lt;= max_elevation]\n        df['Decade'] = f'{start_year}s'\n        decade_data.append(df)\n    \n    # Combine all decades' data\n    combined_df = pd.concat(decade_data, ignore_index=True)\n    \n    # Create the title string including the max elevation\n    title_string = f'Evolution of Elevation-Temperature Relationship in {country}&lt;br&gt;Month {month} (Stations below {max_elevation}m)'\n    \n    # Create faceted scatter plot\n    fig = px.scatter(\n        combined_df,\n        x='STNELEV',\n        y='Temp_Range',\n        facet_col='Decade',\n        facet_col_wrap=2,\n        trendline='ols',\n        hover_data=['NAME', 'LATITUDE', 'LONGITUDE'],\n        labels={\n            'STNELEV': 'Station Elevation (meters)',\n            'Temp_Range': 'Temperature Range (°C)',\n            'NAME': 'Station Name',\n            'Decade': 'Time Period'\n        },\n        title=title_string\n    )\n    \n    # Enhance layout and styling\n    fig.update_layout(\n        height=800,\n        width=1000,\n        title_x=0.5,\n        template='plotly_white',\n        showlegend=False\n    )\n    \n    # Update axes titles for better readability\n    fig.update_xaxes(title_text='Station Elevation (meters)')\n    fig.update_yaxes(title_text='Temperature Range (°C)')\n    \n    return fig\n\n\n\n\nfig = plot_elevation_temp_trends(\"climate.db\", \n                                 \"Germany\", \n                                 ((1980, 1989),(1990, 1999), (2000, 2009), (2010, 2019)), \n                                 3,\n                                max_elevation = 9000)\nfig.show()\n\n\n\n\nFrom the graph, we see that the correlation between elevation and temperature varaibility is not so stable across four decades in Germany in March.\nGreat! Now we have learned to create visualizations using database."
  },
  {
    "objectID": "posts/Data Wrangling and Visualization/index.html#create-a-database",
    "href": "posts/Data Wrangling and Visualization/index.html#create-a-database",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "First, let’s create a database with three tables, temperatures, stations, and countries. They respectively record data on temperatures recorded at climate stations, information about climate stations, and information about countries.\nAs always, we will import necessary libraries at the start.\n\n# import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport inspect\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nFirst, let’s create a database connection and name it climate.db.\n\n# create database connection\nconn = sqlite3.connect(\"climate.db\")\n\nLet’s first download the files.\n\n# load the temperatures data\n\nimport os\n# create folder named \"datafiles\" if it does not exist\nif not os.path.exists(\"datafiles\"): \n    os.mkdir(\"datafiles\")\n\n# download the files\nimport urllib.request\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor interval in intervals:\n    url = f\"https://raw.githubusercontent.com/PIC16B-ucla/25W/main/datasets/noaa-ghcn/decades/{interval}.csv\"\n    urllib.request.urlretrieve(url, f\"datafiles/{interval}.csv\")\n\nTo format our dataframe from the wide format to the long format, we will write a function prepare_df() to help us prepare the dataframe for easier analysis. We will also clean the data by removing rows with temperature of NaN.\n\ndef prepare_df(df):\n    \"\"\"\n    prepares a piece of wide format dataframe into a long format data frame\n    \"\"\"\n    # melt to the long format table\n    df = df.melt(\n        id_vars = [\"ID\", \"Year\"],\n        value_vars = [f\"VALUE{i}\" for i in range(1, 13)],\n        var_name = \"Month\",\n        value_name = \"Temp\"\n    )\n\n    # cleaning month and temp\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    df = df[~np.isnan(df[\"Temp\"])] # remove rows with temperature of NaN \n\n    return df\n\nNext, let’s consolidate all the temperature data from 1901 to 2020 to a single table, which will then be added to the climate database.\n\n# add the temperatures table to the database\nintervals = [f\"{i}-{i+9}\" for i in range(1901, 2020, 10)]\nfor i, interval in enumerate(intervals):\n    filepath = f\"datafiles/{interval}.csv\"\n    df = pd.read_csv(filepath)\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nThe data for stations and countries, respectively stored in a single csv file, will then be loaded and added as tables to the database.\n\n# add the stations table to the database\nstations = pd.read_csv(\"https://raw.githubusercontent.com/PIC16B-ucla/25W/refs/heads/main/datasets/noaa-ghcn/station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n\n# add the countries table to the database\ncountries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n279\n\n\nNow let’s verify whether we have successfully added the three tables to our database. We can also check the names of their columns.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nRemember to close the database connection after finishing constructing the database!\n\nconn.close()"
  },
  {
    "objectID": "posts/Data Wrangling and Visualization/index.html#write-a-query-function",
    "href": "posts/Data Wrangling and Visualization/index.html#write-a-query-function",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Now, let’s write a query function called query_climate_database that makes our workflow easier. This function is designed to help us retrieve information from the climate database all at once, such that we do not need to work on the three tables separately.\nThe function will take in several input parameters that specifies the country and time range for data extraction from the climate database, and will output a dataframe displaying comprehensive information including stations in this country, their location, time for temperature records, and the recorded temperature. Let’s take a close look at the function:\n\nfrom climate_database import query_climate_database\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    input:\n    db_file: the file name for the database\n    country: a string giving the name of a country for which data should be returned.\n    year_begin and year_end: two integers giving the earliest and latest years for which should be returned (inclusive).\n    month: an integer giving the month of the year for which should be returned.\n    \n    output (dataframe):\n    NAME: The station name.\n    LATITUDE: The latitude of the station.\n    LONGITUDE: The longitude of the station.\n    Country: The name of the country in which the station is located.\n    Year: The year in which the reading was taken.\n    Month: The month in which the reading was taken.\n    Temp: The average temperature at the specified station during the specified year and month.\n    \"\"\"\n\n    # connect to database\n    conn = sqlite3.connect(db_file)\n\n    # write sql command using f-strings\n    command = f\"\"\"\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS Country, T.Year, T.Month, T.Temp\n    FROM temperatures T\n    LEFT JOIN stations S\n    ON T.id = S.id\n    LEFT JOIN countries C\n    ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    WHERE C.Name = \"{country}\"\n    AND T.Year &gt;= {year_begin}\n    AND T.Year &lt;= {year_end}\n    AND T.Month = {month}\n    ORDER BY S.NAME, T.Year, T.Month\n    \"\"\"\n\n    df = pd.read_sql_query(command, conn)\n    conn.close()\n    return df\n\n\n\nNow let’s use this function to retrieve some data from the climate database. Looking at India’s temperature data in January from 1980 to 2020, we get:\n\nquery_climate_database(db_file = \"climate.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980\n1\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981\n1\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982\n1\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985\n1\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988\n1\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016\n1\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017\n1\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018\n1\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019\n1\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020\n1\n23.75\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/Data Wrangling and Visualization/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/Data Wrangling and Visualization/index.html#write-a-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Now that we have played around with our database, let’s explore it and create some interesting visualizations.\nOur guiding question will be: How does the average yearly change in temperature vary within a given country?\nIn order to create an interesting visualization to answer this question, let us create a geographic scatterplot in plotly express. Each point on the plot will represent the location of a station, and the color of the point reflects an estimate of the yearly change in temperature during the specific month and time period at that station.\n\nfrom climate_database import temperature_coefficient_plot\nprint(inspect.getsource(temperature_coefficient_plot))\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    inputs:\n    the first five are the same\n    min_obs: the minimum required number of years of data for any given station\n    **kwargs: additional keyword arguments passed to px.scatter_mapbox()\n    \"\"\"\n    \n    # query the database using the function we wrote earlier\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    # filter to have stations which has more than or equal to min_obs\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).filter(lambda group: len(group) &gt;= min_obs)\n\n    # compute the first coefficient of a linear regression model at a given station\n    def coef(data_group):\n        x = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n    \n    coefs = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef).reset_index(name = \"temp_diff\")\n\n    month_name = calendar.month_name[month]\n\n    fig = px.scatter_mapbox(coefs,\n                         lat = \"LATITUDE\",\n                         lon = \"LONGITUDE\",\n                         color = \"temp_diff\",\n                         range_color = [-0.15, 0.15],\n                         hover_name = \"NAME\",\n                         hover_data = {\n                            \"LATITUDE\": \":.3f\",\n                            \"LONGITUDE\": \":.3f\",\n                            \"temp_diff\": \":.3f\"\n                         },\n                         labels = {\n                             \"LATITUDE\": \"LATITUDE\",\n                             \"LONGITUDE\": \"LONGITUDE\",\n                             \"temp_diff\": \"Estimated Yearly Increase (°C)\"\n                         },\n                         title = f\"Yearly temperature increase estimates in {month_name}&lt;br&gt;for {country} stations, {year_begin} to {year_end}\",\n                         **kwargs)\n    \n    return fig\n\n\n\nNow let’s take a look at the January climate data in India from 1980 to 2020. What will it be like when we visualize it using the function we have written?\n\n# assumes you have imported necessary packages\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nLet’s use the same function on another time frame for another country. For example, let’s look at the temperature change in Germany from the year 2000 to 2020 in March. What will our map be like?\n\ncolor_map = px.colors.diverging.Tropic # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"Germany\", 2000, 2020, 3, \n                                   min_obs = 20,\n                                   zoom = 4,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()"
  },
  {
    "objectID": "posts/Data Wrangling and Visualization/index.html#create-two-more-interesting-figures",
    "href": "posts/Data Wrangling and Visualization/index.html#create-two-more-interesting-figures",
    "title": "Data Wrangling and Visualization",
    "section": "",
    "text": "Now that we have created a geographic visualization of the dataset, let’s go ahead and create two more interesting figures from the data.\nLet’s explore the relationship between station elevation and temperature variability. Our analysis will focus on understanding how stations at different elevations experience temperature changes throughout the year, which could provide valuable insights into local climate patterns.\nLet’s create another querying function called query_station_temperature_data to retrieve data from the climate database at once. This function retrieves station-level temperature data for a given country and time, and only selects stations with more than 3 readings in the same month across years.\n\nfrom climate_database import query_station_temperature_data\nprint(inspect.getsource(query_station_temperature_data))\n\ndef query_station_temperature_data(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    input:\n    db_file: the file name for the database.\n    country: a string giving the name of a country.\n    year_begin, year_end: start and end year (inclusive)\n    month: specific month.\n\n    output: a dataframe with the following columns\n    NAME: name of the station\n    LATITUDE, LONGITUDE: location of the station\n    STNELEV: elevation of the station\n    Country: the name of the country in which the station is located.\n    Year: the Year for the reading\n    Month: the Month for the reading\n    Temp_Range: range of temperature in the station\n    Num_Readings: number of readings for the station\n    \"\"\"\n\n    # connect to database\n    conn = sqlite3.connect(db_file)\n\n\n    # write sql command using f-strings\n    # only select stations with more than 3 readings in a same month across years\n    command = f\"\"\"\n    WITH yearly_station_counts AS (\n        SELECT S.NAME, COUNT(DISTINCT T.Year) as year_count\n        FROM temperatures T\n        LEFT JOIN stations S ON T.id = S.id\n        LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.Name = \"{country}\"\n        AND T.Year &gt;= {year_begin}\n        AND T.Year &lt;= {year_end}\n        AND T.Month = {month}\n        GROUP BY S.NAME\n        HAVING year_count &gt;= 3\n    )\n    SELECT \n        S.NAME, S.LATITUDE, S.LONGITUDE, S.STNELEV,\n        C.Name AS Country, \n        T.Month,\n        MAX(T.Temp) - MIN(T.Temp) AS Temp_Range\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    INNER JOIN yearly_station_counts ysc ON S.NAME = ysc.NAME\n    WHERE C.Name = \"{country}\"\n    AND T.Year &gt;= {year_begin}\n    AND T.Year &lt;= {year_end}\n    AND T.Month = {month}\n    GROUP BY S.NAME, S.LATITUDE, S.LONGITUDE, S.STNELEV\n    \"\"\"\n\n    df = pd.read_sql_query(command, conn)\n    conn.close()\n    return df\n\n\n\nLet’s check the output of this query function using the same example we have before.\n\nquery_station_temperature_data(db_file = \"climate.db\",\n                               country = \"India\", \n                               year_begin = 1980,\n                               year_end = 2000,\n                               month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nSTNELEV\nCountry\nMonth\nTemp_Range\n\n\n\n\n0\nAGARTALA\n23.8830\n91.2500\n16.0\nIndia\n1\n2.95\n\n\n1\nAGRA\n27.1667\n78.0333\n168.0\nIndia\n1\n2.20\n\n\n2\nAHMADABAD\n23.0670\n72.6330\n55.0\nIndia\n1\n3.54\n\n\n3\nAKOLA\n20.7000\n77.0330\n282.0\nIndia\n1\n4.60\n\n\n4\nAKOLA\n20.7000\n77.0670\n305.0\nIndia\n1\n3.63\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n92\nTIRUCHCHIRAPALLI\n10.7670\n78.7170\n88.0\nIndia\n1\n2.79\n\n\n93\nTRIVANDRUM\n8.5000\n77.0000\n64.0\nIndia\n1\n2.10\n\n\n94\nVARANASI_BABATPUR\n25.4500\n82.8670\n85.0\nIndia\n1\n0.93\n\n\n95\nVERAVAL\n20.9000\n70.3670\n8.0\nIndia\n1\n3.47\n\n\n96\nVISHAKHAPATNAM\n17.7170\n83.2330\n3.0\nIndia\n1\n3.00\n\n\n\n\n97 rows × 7 columns\n\n\n\nWith the help of the querying function, we can now answer the first question: How does temperature variability differ between high-elevation and low-elevation stations in a country in a given month?\nWe will create an interactive box plot. The X-axis will be the elevation bins, whereas the Y-axis will represent temperature range per station.\n\nfrom climate_database import plot_elevation_temp_variability\nprint(inspect.getsource(plot_elevation_temp_variability))\n\ndef plot_elevation_temp_variability(db_file, country, year_begin, year_end, month, num_bins=5):\n\n    \"\"\"\n    input:\n    db_file: database file path\n    country: country name\n    year_begin, year_end: time period to analyze\n    month: month to analyze (1-12)\n    num_bins: number of elevation bins to create\n    \"\"\"\n    # Get the data\n    df = query_station_temperature_data(db_file, country, year_begin, year_end, month)\n    \n    # Create elevation bins\n    df['Elevation_Bin'] = pd.qcut(df['STNELEV'], \n                                q=num_bins, \n                                labels=[f'{int(x.left)}-{int(x.right)}m' \n                                       for x in pd.qcut(df['STNELEV'], q=num_bins).unique()])\n    \n    # Create box plot\n    fig = px.box(\n        df,\n        x='Elevation_Bin',\n        y='Temp_Range',\n        points='all',  # Show all points\n        hover_data=['NAME', 'STNELEV'],  # Show station details on hover\n        labels={\n            'Elevation_Bin': 'Elevation Range',\n            'Temp_Range': 'Temperature Range (°C)',\n            'NAME': 'Station Name',\n            'STNELEV': 'Exact Elevation (m)'\n        },\n        title=f'Temperature Variability Across Elevation Ranges in {country}&lt;br&gt;{year_begin} to {year_end}, Month {month}'\n    )\n    \n    # Enhance layout\n    fig.update_layout(\n        height=600,\n        width=900,\n        title_x=0.5,\n        template='plotly_white',\n        xaxis_title=f'Elevation Ranges (divided into {num_bins} groups)'\n    )\n    \n    return fig\n\n\n\nLet’s generate the graph:\n\nfig = plot_elevation_temp_variability(\"climate.db\", \"India\", 1980, 2020, 1, num_bins=5)\nfig.show()\n\n\n\n\nGreat. Now let’s answer our second question: How does the correlation between elevation and temperature variability change over time?\nWe will created a faceted scatter plot, where the x-axis is the elvation, and y-axis represents the temperature range. We will also add a trendline showing linear regression per decade.\n\nfrom climate_database import plot_elevation_temp_trends\nprint(inspect.getsource(plot_elevation_temp_trends))\n\ndef plot_elevation_temp_trends(db_file, country, decade_ranges, month, max_elevation = 9000):\n    \"\"\"\n    input:\n    db_file: database file path\n    country: country name\n    decade_ranges: list of tuples, each containing (start_year, end_year) for a decade\n    month: month to analyze (1-12)\n    \"\"\"\n\n    # Initialize list to store data from each decade\n    decade_data = []\n    \n    # Collect data for each decade\n    for start_year, end_year in decade_ranges:\n        df = query_station_temperature_data(db_file, country, start_year, end_year, month)\n        # Filter out stations with unrealistic elevations\n        df = df[df['STNELEV'] &lt;= max_elevation]\n        df['Decade'] = f'{start_year}s'\n        decade_data.append(df)\n    \n    # Combine all decades' data\n    combined_df = pd.concat(decade_data, ignore_index=True)\n    \n    # Create the title string including the max elevation\n    title_string = f'Evolution of Elevation-Temperature Relationship in {country}&lt;br&gt;Month {month} (Stations below {max_elevation}m)'\n    \n    # Create faceted scatter plot\n    fig = px.scatter(\n        combined_df,\n        x='STNELEV',\n        y='Temp_Range',\n        facet_col='Decade',\n        facet_col_wrap=2,\n        trendline='ols',\n        hover_data=['NAME', 'LATITUDE', 'LONGITUDE'],\n        labels={\n            'STNELEV': 'Station Elevation (meters)',\n            'Temp_Range': 'Temperature Range (°C)',\n            'NAME': 'Station Name',\n            'Decade': 'Time Period'\n        },\n        title=title_string\n    )\n    \n    # Enhance layout and styling\n    fig.update_layout(\n        height=800,\n        width=1000,\n        title_x=0.5,\n        template='plotly_white',\n        showlegend=False\n    )\n    \n    # Update axes titles for better readability\n    fig.update_xaxes(title_text='Station Elevation (meters)')\n    fig.update_yaxes(title_text='Temperature Range (°C)')\n    \n    return fig\n\n\n\n\nfig = plot_elevation_temp_trends(\"climate.db\", \n                                 \"Germany\", \n                                 ((1980, 1989),(1990, 1999), (2000, 2009), (2010, 2019)), \n                                 3,\n                                max_elevation = 9000)\nfig.show()\n\n\n\n\nFrom the graph, we see that the correlation between elevation and temperature varaibility is not so stable across four decades in Germany in March.\nGreat! Now we have learned to create visualizations using database."
  },
  {
    "objectID": "posts/Web Scraping with Scrapy/index.html",
    "href": "posts/Web Scraping with Scrapy/index.html",
    "title": "Create A Movie Recommender through Web Scraping",
    "section": "",
    "text": "In this tutorial, we’ll explore how to build a movie recommendation system using web scraping with Python’s Scrapy framework. While there are various established approaches to building recommender systems (such as content-based filtering and collaborative filtering), we’ll take an innovative approach by leveraging existing movie databases online. Our method will focus on finding movies that share the most actors with a target movie, using this cast overlap as a basis for generating relevant recommendations.\n\n\nFor this tutorial, we will use TMDB (https://www.themoviedb.org/) to access comprehensive movie data, including cast and crew information.\nFirst, let’s create a file in the spiders directory called tmdb_spider.py. We will add the following lines to the file:\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThis creates a spider named tmdb_spider that will start scraping from a specified TMDB movie page. The subdir parameter allows us to specify which movie’s page to begin scraping.\nThis spider follows a three-stage scraping process. It begins at a movie’s main page, and then navigates to the Full Cast & Crew page. Once it reaches the cast page, it will access the individual actor pages. For each actor, it then collects a list of other movies or TV shows they have appeared in.\nWe’re going to create three parsing methods for each stage of the scraping.\n\n\nThe first parsing method we write in the spider will assume that we start on a movie page, and then navigate to the Full Cast & Crew Page. We write it as the follows:\n\ndef parse(self, response):\n        \"\"\"\n        parse the movie page and navigate to the full cast & crew page\n        arguments: the response object containing the movie page HTML\n        yields a request to follow the cast page URL, with parse_full_credits as callback\n        \"\"\"\n        \n        cast_page_link = response.css('.new_button a::attr(href)').get()\n        yield response.follow(cast_page_link, callback = self.parse_full_credits) # follow URL of the cast page\n\nThis method appends “/cast” to the movie page URL to access the Cast & Crew page. When called, it yields a new request that Scrapy will follow, using parse_full_credits as the callback method to process the resulting page.\n\n\n\nThe second parsing method, parse_full_credits, will access the individual pages of actors (not including crew members) in the specified movie.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    parses the full cast and crew page and navigate to actors' individual pages\n    arguments: the response object containing the cast & crew page HTML\n    yields: request to follow each actors' page URL, with parse_actor_page as callback\n    \"\"\"\n    \n    # access the relative URLs for actors' individual pages\n    actor_pages = response.css('ol.people.credits:not(.crew) li &gt; a::attr(href)').getall()\n\n    # yield new request for each actor page\n    for actor in actor_pages:\n        yield response.follow(actor, callback = self.parse_actor_page)\n\nIn this method, we first use a CSS selector to find all the relative URLs for actors’ individual pages. We use the symbol ‘&gt;’ to make sure that we only select a elements that are direct children of li elements. It then iterates through each of these URLs and create a new request for each actor’s page, which will then be processed by the parse_actor_page method that we will look into next.\n\n\n\nNow we have reached the third stage of scraping! We will write the third parsing method, parse_actor_page. This method collects each actor’s name and their complete list of acting roles, generating a dictionary for each unique movie or TV show they’ve appeared in.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    parse the actor page and return a dictionary for actor name and movie/tv names\n    argument: the response object containing the actor's page HTML\n    yield a dictionary of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name},\n    which records a list of unique movie/tv names where the actor has acted\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get()\n\n    # extract the categories the actor have worked in (acting, production, ...)\n    categories = response.css('div.credits_list h3::text').getall()\n\n    # extract the table of credits for all categories\n    all_titles = response.css('div.credits_list table.credits')\n\n    for i, category in enumerate(categories):\n        if category == 'Acting': # only check the acting categories\n            titles = all_titles[i].css('table.credit_group tr a.tool:tip bdi::text').getall()\n\n            # get unique titles using set()\n            unique_titles = list(set(titles))\n\n            for title in unique_titles:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n            break # break when category is no longer \"acting\"\n\nThis method first extracts the actor’s name, the categories the actor have worked in (acting, production…), and the table of credits for all categories. It then checks if the category is “Acting”, and then extract the titles of movies/TV under the acting credits. The function set() is used to get unique titles, and from there, we will yield a dictionary containing both the actor’s name and the movie/TV title.\nThis method will iterate through all actors of the specified movie, so we will get a long list of all the movie/TV titles that each of the actors of the movie have acted in.\n\n\n\n\nOne of my favorite movies is Mulholland Drive (2001) directed by David Lynch. It is a masterpiece of surrealist cinema that blends mystery, psychological thriller, and noir elements to create an enigmatic narrative about dreams and reality in Hollywood.\nThe link to this movie page is https://www.themoviedb.org/movie/1018-mulholland-drive, and let’s test our scraper to see if it works well in generating movie recommendations!\nWe will run the following command in the terminal (make sure you are in the same directory as the spider):\nscrapy crawl tmdb_spider -o results.csv -a subdir=1018-mulholland-drive\nThis command will generate a CSV file named results.csv in your directory. The file will contain a comprehensive list of actors from Mulholland Drive and their corresponding filmographies, which we can analyze to identify potential movie recommendations based on cast overlap.\n\n\nLet’s evaluate our scraper’s effectiveness in generating movie recommendations by analyzing the cast connections.\nFirst, let’s compute a sorted list with the top movies and TC shows that share actors with Mulholland Drive. It will have two columns: “move names” and “number of shared actors”.\nLet’s first import necessary libraries and load the data:\n\nimport pandas as pd\nmovie = pd.read_csv(\"TMDB_scraper/TMDB_scraper/spiders/results.csv\")\n\nNext, we will group the movie dataframe by movie name and count the number of unique actors in each of them. We will then sort the dataframe by the number of shared actors in descending order:\n\n# Filter out Mulholland Drive to only get other movies\nother_movies = movie[~movie['movie_or_TV_name'].isin(['Mulholland Dr.', 'Mulholland Drive'])]\n\n# Group by movie name and count unique actors\nmovie_connections = other_movies.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\n# Rename columns to match desired format\nmovie_connections.columns = ['movie names', 'number of shared actors']\n\n# Sort by number of shared actors in descending order\nmovie_recommendations = movie_connections.sort_values(by='number of shared actors', ascending=False)\n\nmovie_recommendations\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n273\nCSI: Crime Scene Investigation\n8\n\n\n2037\nTwin Peaks\n8\n\n\n836\nJAG\n7\n\n\n345\nCold Case\n7\n\n\n983\nMacGyver\n6\n\n\n...\n...\n...\n\n\n764\nHome of the Giants\n1\n\n\n763\nHome and Away\n1\n\n\n762\nHome Improvement\n1\n\n\n761\nHome\n1\n\n\n2185\nseaQuest DSV\n1\n\n\n\n\n2186 rows × 2 columns\n\n\n\n\n\n\nLet’s visualize the result we get above using a bar plot. For this visualization, we will look at the top 10 movies recommended by the scraper.\n\n# import necessary libraries\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nfrom plotly import express as px\n\nfig = px.bar(movie_recommendations.head(10),\n            x='number of shared actors',\n            y='movie names',\n            orientation='h',\n            title='Top 10 Movies/TV Shows Recommended by the Scraper')\n\nfig.show()\n\n\n\n\nGreat! Do these recommended movies meet your expectations? Or will there be better ways for us to generate better movie recommendations?"
  },
  {
    "objectID": "posts/Web Scraping with Scrapy/index.html#set-up-the-spider",
    "href": "posts/Web Scraping with Scrapy/index.html#set-up-the-spider",
    "title": "Create A Movie Recommender through Web Scraping",
    "section": "",
    "text": "For this tutorial, we will use TMDB (https://www.themoviedb.org/) to access comprehensive movie data, including cast and crew information.\nFirst, let’s create a file in the spiders directory called tmdb_spider.py. We will add the following lines to the file:\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThis creates a spider named tmdb_spider that will start scraping from a specified TMDB movie page. The subdir parameter allows us to specify which movie’s page to begin scraping.\nThis spider follows a three-stage scraping process. It begins at a movie’s main page, and then navigates to the Full Cast & Crew page. Once it reaches the cast page, it will access the individual actor pages. For each actor, it then collects a list of other movies or TV shows they have appeared in.\nWe’re going to create three parsing methods for each stage of the scraping.\n\n\nThe first parsing method we write in the spider will assume that we start on a movie page, and then navigate to the Full Cast & Crew Page. We write it as the follows:\n\ndef parse(self, response):\n        \"\"\"\n        parse the movie page and navigate to the full cast & crew page\n        arguments: the response object containing the movie page HTML\n        yields a request to follow the cast page URL, with parse_full_credits as callback\n        \"\"\"\n        \n        cast_page_link = response.css('.new_button a::attr(href)').get()\n        yield response.follow(cast_page_link, callback = self.parse_full_credits) # follow URL of the cast page\n\nThis method appends “/cast” to the movie page URL to access the Cast & Crew page. When called, it yields a new request that Scrapy will follow, using parse_full_credits as the callback method to process the resulting page.\n\n\n\nThe second parsing method, parse_full_credits, will access the individual pages of actors (not including crew members) in the specified movie.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    parses the full cast and crew page and navigate to actors' individual pages\n    arguments: the response object containing the cast & crew page HTML\n    yields: request to follow each actors' page URL, with parse_actor_page as callback\n    \"\"\"\n    \n    # access the relative URLs for actors' individual pages\n    actor_pages = response.css('ol.people.credits:not(.crew) li &gt; a::attr(href)').getall()\n\n    # yield new request for each actor page\n    for actor in actor_pages:\n        yield response.follow(actor, callback = self.parse_actor_page)\n\nIn this method, we first use a CSS selector to find all the relative URLs for actors’ individual pages. We use the symbol ‘&gt;’ to make sure that we only select a elements that are direct children of li elements. It then iterates through each of these URLs and create a new request for each actor’s page, which will then be processed by the parse_actor_page method that we will look into next.\n\n\n\nNow we have reached the third stage of scraping! We will write the third parsing method, parse_actor_page. This method collects each actor’s name and their complete list of acting roles, generating a dictionary for each unique movie or TV show they’ve appeared in.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    parse the actor page and return a dictionary for actor name and movie/tv names\n    argument: the response object containing the actor's page HTML\n    yield a dictionary of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name},\n    which records a list of unique movie/tv names where the actor has acted\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get()\n\n    # extract the categories the actor have worked in (acting, production, ...)\n    categories = response.css('div.credits_list h3::text').getall()\n\n    # extract the table of credits for all categories\n    all_titles = response.css('div.credits_list table.credits')\n\n    for i, category in enumerate(categories):\n        if category == 'Acting': # only check the acting categories\n            titles = all_titles[i].css('table.credit_group tr a.tool:tip bdi::text').getall()\n\n            # get unique titles using set()\n            unique_titles = list(set(titles))\n\n            for title in unique_titles:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n            break # break when category is no longer \"acting\"\n\nThis method first extracts the actor’s name, the categories the actor have worked in (acting, production…), and the table of credits for all categories. It then checks if the category is “Acting”, and then extract the titles of movies/TV under the acting credits. The function set() is used to get unique titles, and from there, we will yield a dictionary containing both the actor’s name and the movie/TV title.\nThis method will iterate through all actors of the specified movie, so we will get a long list of all the movie/TV titles that each of the actors of the movie have acted in."
  },
  {
    "objectID": "posts/Web Scraping with Scrapy/index.html#using-the-scraper-for-movie-recommendation",
    "href": "posts/Web Scraping with Scrapy/index.html#using-the-scraper-for-movie-recommendation",
    "title": "Create A Movie Recommender through Web Scraping",
    "section": "",
    "text": "One of my favorite movies is Mulholland Drive (2001) directed by David Lynch. It is a masterpiece of surrealist cinema that blends mystery, psychological thriller, and noir elements to create an enigmatic narrative about dreams and reality in Hollywood.\nThe link to this movie page is https://www.themoviedb.org/movie/1018-mulholland-drive, and let’s test our scraper to see if it works well in generating movie recommendations!\nWe will run the following command in the terminal (make sure you are in the same directory as the spider):\nscrapy crawl tmdb_spider -o results.csv -a subdir=1018-mulholland-drive\nThis command will generate a CSV file named results.csv in your directory. The file will contain a comprehensive list of actors from Mulholland Drive and their corresponding filmographies, which we can analyze to identify potential movie recommendations based on cast overlap.\n\n\nLet’s evaluate our scraper’s effectiveness in generating movie recommendations by analyzing the cast connections.\nFirst, let’s compute a sorted list with the top movies and TC shows that share actors with Mulholland Drive. It will have two columns: “move names” and “number of shared actors”.\nLet’s first import necessary libraries and load the data:\n\nimport pandas as pd\nmovie = pd.read_csv(\"TMDB_scraper/TMDB_scraper/spiders/results.csv\")\n\nNext, we will group the movie dataframe by movie name and count the number of unique actors in each of them. We will then sort the dataframe by the number of shared actors in descending order:\n\n# Filter out Mulholland Drive to only get other movies\nother_movies = movie[~movie['movie_or_TV_name'].isin(['Mulholland Dr.', 'Mulholland Drive'])]\n\n# Group by movie name and count unique actors\nmovie_connections = other_movies.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\n# Rename columns to match desired format\nmovie_connections.columns = ['movie names', 'number of shared actors']\n\n# Sort by number of shared actors in descending order\nmovie_recommendations = movie_connections.sort_values(by='number of shared actors', ascending=False)\n\nmovie_recommendations\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n273\nCSI: Crime Scene Investigation\n8\n\n\n2037\nTwin Peaks\n8\n\n\n836\nJAG\n7\n\n\n345\nCold Case\n7\n\n\n983\nMacGyver\n6\n\n\n...\n...\n...\n\n\n764\nHome of the Giants\n1\n\n\n763\nHome and Away\n1\n\n\n762\nHome Improvement\n1\n\n\n761\nHome\n1\n\n\n2185\nseaQuest DSV\n1\n\n\n\n\n2186 rows × 2 columns\n\n\n\n\n\n\nLet’s visualize the result we get above using a bar plot. For this visualization, we will look at the top 10 movies recommended by the scraper.\n\n# import necessary libraries\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nfrom plotly import express as px\n\nfig = px.bar(movie_recommendations.head(10),\n            x='number of shared actors',\n            y='movie names',\n            orientation='h',\n            title='Top 10 Movies/TV Shows Recommended by the Scraper')\n\nfig.show()\n\n\n\n\nGreat! Do these recommended movies meet your expectations? Or will there be better ways for us to generate better movie recommendations?"
  },
  {
    "objectID": "posts/Web Development with Dash/index.html",
    "href": "posts/Web Development with Dash/index.html",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "In this tutorial, we’ll walk through the process of building a simple web application using Dash by Plotly. The app we are creating is a message bank, which allows users to submit messages and view a sample of messages stored in a database. By the end of this tutorial, you will have a functional web app that demonstrates the use of Dash callbacks, database management, and basic styling.\n\n\nFirst, let’s build the dash web app before adding functionalities.\nWe’ll start by importing the required libraries and initializing the app. Then, we will create a message bank to store users’ daily mood quotes:\n\nfrom dash import Dash, html, dcc, Input, Output, State\napp = Dash(__name__)\napp.title = \"Message Bank: Daily Mood Quotes\"\n\nNext, we will create the app layout. It consists of two sections: submit and view.\nThe submission functionality will have three user interface elements: - a text box for submitting a message - a text box for submitting the name of the user - a “submit” button\nThe view functionality will have two user interface elements: - a section for displaying random messages - an “update” button\nLet’s also apply some styling to the layout, including fonts, colors, margins, and more.\n\napp.layout = html.Div([\n    html.H1(\"Message Bank: Daily Mood Quotes\", \n            style={\n                \"textAlign\": \"center\",\n                \"color\": \"#084a8c\",\n                \"fontFamily\": \"'Roboto', sans-serif\",\n                \"marginBottom\": \"20px\"\n    }),\n    \n    # Submission Section\n    html.H2(\"Submit\", style={\n        \"color\": \"#7848a8\",\n        \"fontFamily\": \"'Roboto', sans-serif\"\n    }),\n    html.Label(\"Your message:\", style={\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n        }), \n    html.Br(),\n    dcc.Textarea(id=\"message\", style = {\n        \"width\": \"50%\",\n        \"height\": \"100px\",\n        \"verticalAlign\": \"top\"\n    }),\n    html.Br(),\n    html.Label(\"Your Name or Handle:\", style={\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n        }),\n    html.Br(),\n    dcc.Input(id=\"handle\", type=\"text\"),\n    html.Br(),\n    html.Button(\"Submit\", id=\"submit-button\",\n                style={\n        \"backgroundColor\": \"#e0d1f0\",\n        \"color\": \"white\",\n        \"padding\": \"10px 20px\",\n        \"border\": \"none\",\n        \"borderRadius\": \"25px\",\n        \"cursor\": \"pointer\",\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"fontSize\": \"16px\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n    }),\n    html.Div(id=\"submit-confirmation\", style={\"fontFamily\": \"'Roboto', sans-serif\"}),\n    \n    # View Messages Section\n    html.Hr(),\n    html.H2(\"View\", style={\n        \"color\": \"#7848a8\",\n        \"fontFamily\": \"'Roboto', sans-serif\"\n    }),\n    html.Div(id=\"message-display\",\n             style = {\"fontFamily\": \"'Roboto', sans-serif\"}),\n    html.Button(\"Update\", id=\"update-button\", style={\n        \"backgroundColor\": \"#e0d1f0\",\n        \"color\": \"white\",\n        \"padding\": \"10px 20px\",\n        \"border\": \"none\",\n        \"borderRadius\": \"25px\",\n        \"cursor\": \"pointer\",\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"fontSize\": \"16px\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n    }),  \n])\n\n\n\n\nIn this section, we will create a database to store the messages.\nFor database management in the app, we will create two python functions. The first function is get_message_db(). It handles the creation of the database and the messages table if they don’t already exist.\n\nimport sqlite3\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"\n    create or connect to the sqlite database and ensure the messages table exists\n    retun: the database connection\n    \"\"\"\n    global message_db\n    if message_db:\n        return message_db\n    \n    else:\n        # connect to database (or create it if it doesn't exist)\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n        # create the messages table if it doesn't exist\n        cmd = \"\"\"\n            CREATE TABLE IF NOT EXISTS messages(\n                handle TEXT,\n                message TEXT\n            )\n            \"\"\"\n        \n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        return message_db\n\nThe function above first check whether there is a database called message_db defined in the global scope. If not, it will connect to that database and assign it to the global variable message_db.\nIt will also check whether a table called messages exists in the database, and create it if not. Using SQL command, we create a table with two columns: handle and message. At the end, the function returns the connection message_db.\nThe second function is insert_message(handle, message), which inserts the message into the database.\n\ndef insert_message(handle, message):\n    \"\"\"\n    insert a new message into the messages database\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    cmd = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    cursor.execute(cmd, (handle, message))\n    \n    db.commit() # save the row insertion\n    global message_db\n    message_db = None # set the global message_db to none\n    db.close() # close the database connection\n\nThe function above uses a cursor to insert the message into the message database. We use parameterized query with ? as placeholders, which properly handles the string quotes.\nAt the end of the function, we run db.commit() after row insertion to ensure the change has been saved, and we will also close the database connection. Note that a column called rowid is automatically generated by default, which gives an integer index to each row we add to the database.\n\n\n\nNow that we have created functions for database management, we will add a submit functionality to our web app.\nTo enable interaction with a input component and change the resulting output component, we will need to create a callback function submit() to update the components.\n\n@app.callback(\n    Output(\"submit-confirmation\", \"children\"), # the output will update this div element\n    Input(\"submit-button\", \"n_clicks\"),\n    State(\"handle\", \"value\"),\n    State(\"message\", \"value\"),\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    \"\"\"\n    insert the submitted message into the database and display a confirmation line\n    \"\"\"\n    if handle and message:\n        insert_message(handle, message)\n        return \"Thank you for your submission!\" # successful submission\n    else:\n        return \"Error: please check if any field is empty!\" # error occurs\n\nThis function inserts the submitted message into the messages database and display a confirmation line if submission is successful, and prints an error message if it failed.\nThe callback specifies each specific input and output components, as well as their specific properties. For example, the output will be the “submit-confirmation” component and we will update its children html element.\n\n\n\nAfter enabling the submit functionality, we will add the second component, viewing submissions. In this section, we will write two other functions to enable message display.\nFirst, we will write a function called random_messages(n), which will return a collection of n random messages from the message_db, or fewer if necessary. We have set a default number of 5 messages to be displayed.\n\ndef random_messages(n=5):\n    \"\"\"\n    return a collection of n random messages from the database\n    or fewer if necessary\n    \"\"\"\n\n    db = get_message_db()\n    cursor = db.cursor()\n\n    cmd = \"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\" # parameterized query\n    cursor.execute(cmd, (n,))\n    messages = cursor.fetchall()\n\n    db.close()\n    global message_db\n    message_db = None # set the global message_db to none\n    return messages\n\nNext, we will write a callback function called view() to display random messages. These messages will be displayed in the section with the component id “message-display”.\n\n@app.callback(\n        Output(\"message-display\", \"children\"),\n        Input(\"update-button\", \"n_clicks\"),\n        prevent_initial_call=True\n)\ndef view(n_clicks):\n    \"\"\"\n    display random messages\n    \"\"\"\n    messages = random_messages()\n\n    # iterate through each of the message\n    messages_list = []\n    for handle, message in messages:\n        messages_list.append(html.P([\n            message,\n            html.Br(),\n            \"- \" + handle\n        ]))\n\n    return messages_list\n\nThis function first calls random_messages() to grab some messages and display them using a loop. This function is triggered when the “update” button is pressed, as specified in the callback.\n\n\n\nNow that we’ve set up all the features and layout of our website, let’s run the app and test its functionality to ensure everything works as expected.\n\nif __name__ == \"__main__\":\n    app.run_server(port=8050, debug=True)\n\n\n        \n        \n\n\nFirst, let’s test the submission function. It works perfectly, displaying a thank you message after a successful submission. \nNext, let’s check the message display feature. We’ve submitted seven messages, each labeled with keywords such as “one”, “two”, “three”, and so on, to indicate the order of submission. Since we’ve capped the display to a maximum of five messages, clicking the update button will randomly select five messages from the database.\n\n\n\nScrenshot of Message Display\n\n\nGreat! We see that the web app is working as desired. With these functions tested and confirmed, our web app is now ready for deployment! :)"
  },
  {
    "objectID": "posts/Web Development with Dash/index.html#building-the-web-app-using-dash",
    "href": "posts/Web Development with Dash/index.html#building-the-web-app-using-dash",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "First, let’s build the dash web app before adding functionalities.\nWe’ll start by importing the required libraries and initializing the app. Then, we will create a message bank to store users’ daily mood quotes:\n\nfrom dash import Dash, html, dcc, Input, Output, State\napp = Dash(__name__)\napp.title = \"Message Bank: Daily Mood Quotes\"\n\nNext, we will create the app layout. It consists of two sections: submit and view.\nThe submission functionality will have three user interface elements: - a text box for submitting a message - a text box for submitting the name of the user - a “submit” button\nThe view functionality will have two user interface elements: - a section for displaying random messages - an “update” button\nLet’s also apply some styling to the layout, including fonts, colors, margins, and more.\n\napp.layout = html.Div([\n    html.H1(\"Message Bank: Daily Mood Quotes\", \n            style={\n                \"textAlign\": \"center\",\n                \"color\": \"#084a8c\",\n                \"fontFamily\": \"'Roboto', sans-serif\",\n                \"marginBottom\": \"20px\"\n    }),\n    \n    # Submission Section\n    html.H2(\"Submit\", style={\n        \"color\": \"#7848a8\",\n        \"fontFamily\": \"'Roboto', sans-serif\"\n    }),\n    html.Label(\"Your message:\", style={\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n        }), \n    html.Br(),\n    dcc.Textarea(id=\"message\", style = {\n        \"width\": \"50%\",\n        \"height\": \"100px\",\n        \"verticalAlign\": \"top\"\n    }),\n    html.Br(),\n    html.Label(\"Your Name or Handle:\", style={\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n        }),\n    html.Br(),\n    dcc.Input(id=\"handle\", type=\"text\"),\n    html.Br(),\n    html.Button(\"Submit\", id=\"submit-button\",\n                style={\n        \"backgroundColor\": \"#e0d1f0\",\n        \"color\": \"white\",\n        \"padding\": \"10px 20px\",\n        \"border\": \"none\",\n        \"borderRadius\": \"25px\",\n        \"cursor\": \"pointer\",\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"fontSize\": \"16px\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n    }),\n    html.Div(id=\"submit-confirmation\", style={\"fontFamily\": \"'Roboto', sans-serif\"}),\n    \n    # View Messages Section\n    html.Hr(),\n    html.H2(\"View\", style={\n        \"color\": \"#7848a8\",\n        \"fontFamily\": \"'Roboto', sans-serif\"\n    }),\n    html.Div(id=\"message-display\",\n             style = {\"fontFamily\": \"'Roboto', sans-serif\"}),\n    html.Button(\"Update\", id=\"update-button\", style={\n        \"backgroundColor\": \"#e0d1f0\",\n        \"color\": \"white\",\n        \"padding\": \"10px 20px\",\n        \"border\": \"none\",\n        \"borderRadius\": \"25px\",\n        \"cursor\": \"pointer\",\n        \"fontFamily\": \"'Roboto', sans-serif\",\n        \"fontSize\": \"16px\",\n        \"marginTop\": \"10px\",\n        \"marginBottom\": \"10px\"\n    }),  \n])"
  },
  {
    "objectID": "posts/Web Development with Dash/index.html#creating-the-messages-database",
    "href": "posts/Web Development with Dash/index.html#creating-the-messages-database",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "In this section, we will create a database to store the messages.\nFor database management in the app, we will create two python functions. The first function is get_message_db(). It handles the creation of the database and the messages table if they don’t already exist.\n\nimport sqlite3\n\nmessage_db = None\n\ndef get_message_db():\n    \"\"\"\n    create or connect to the sqlite database and ensure the messages table exists\n    retun: the database connection\n    \"\"\"\n    global message_db\n    if message_db:\n        return message_db\n    \n    else:\n        # connect to database (or create it if it doesn't exist)\n        message_db = sqlite3.connect(\"messages_db.sqlite\", check_same_thread=False)\n\n        # create the messages table if it doesn't exist\n        cmd = \"\"\"\n            CREATE TABLE IF NOT EXISTS messages(\n                handle TEXT,\n                message TEXT\n            )\n            \"\"\"\n        \n        cursor = message_db.cursor()\n        cursor.execute(cmd)\n        return message_db\n\nThe function above first check whether there is a database called message_db defined in the global scope. If not, it will connect to that database and assign it to the global variable message_db.\nIt will also check whether a table called messages exists in the database, and create it if not. Using SQL command, we create a table with two columns: handle and message. At the end, the function returns the connection message_db.\nThe second function is insert_message(handle, message), which inserts the message into the database.\n\ndef insert_message(handle, message):\n    \"\"\"\n    insert a new message into the messages database\n    \"\"\"\n    db = get_message_db()\n    cursor = db.cursor()\n    \n    cmd = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"\n    cursor.execute(cmd, (handle, message))\n    \n    db.commit() # save the row insertion\n    global message_db\n    message_db = None # set the global message_db to none\n    db.close() # close the database connection\n\nThe function above uses a cursor to insert the message into the message database. We use parameterized query with ? as placeholders, which properly handles the string quotes.\nAt the end of the function, we run db.commit() after row insertion to ensure the change has been saved, and we will also close the database connection. Note that a column called rowid is automatically generated by default, which gives an integer index to each row we add to the database."
  },
  {
    "objectID": "posts/Web Development with Dash/index.html#enable-submissions-through-callback-function",
    "href": "posts/Web Development with Dash/index.html#enable-submissions-through-callback-function",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "Now that we have created functions for database management, we will add a submit functionality to our web app.\nTo enable interaction with a input component and change the resulting output component, we will need to create a callback function submit() to update the components.\n\n@app.callback(\n    Output(\"submit-confirmation\", \"children\"), # the output will update this div element\n    Input(\"submit-button\", \"n_clicks\"),\n    State(\"handle\", \"value\"),\n    State(\"message\", \"value\"),\n    prevent_initial_call=True\n)\ndef submit(n_clicks, handle, message):\n    \"\"\"\n    insert the submitted message into the database and display a confirmation line\n    \"\"\"\n    if handle and message:\n        insert_message(handle, message)\n        return \"Thank you for your submission!\" # successful submission\n    else:\n        return \"Error: please check if any field is empty!\" # error occurs\n\nThis function inserts the submitted message into the messages database and display a confirmation line if submission is successful, and prints an error message if it failed.\nThe callback specifies each specific input and output components, as well as their specific properties. For example, the output will be the “submit-confirmation” component and we will update its children html element."
  },
  {
    "objectID": "posts/Web Development with Dash/index.html#enabling-viewing-through-callback-function",
    "href": "posts/Web Development with Dash/index.html#enabling-viewing-through-callback-function",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "After enabling the submit functionality, we will add the second component, viewing submissions. In this section, we will write two other functions to enable message display.\nFirst, we will write a function called random_messages(n), which will return a collection of n random messages from the message_db, or fewer if necessary. We have set a default number of 5 messages to be displayed.\n\ndef random_messages(n=5):\n    \"\"\"\n    return a collection of n random messages from the database\n    or fewer if necessary\n    \"\"\"\n\n    db = get_message_db()\n    cursor = db.cursor()\n\n    cmd = \"SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?\" # parameterized query\n    cursor.execute(cmd, (n,))\n    messages = cursor.fetchall()\n\n    db.close()\n    global message_db\n    message_db = None # set the global message_db to none\n    return messages\n\nNext, we will write a callback function called view() to display random messages. These messages will be displayed in the section with the component id “message-display”.\n\n@app.callback(\n        Output(\"message-display\", \"children\"),\n        Input(\"update-button\", \"n_clicks\"),\n        prevent_initial_call=True\n)\ndef view(n_clicks):\n    \"\"\"\n    display random messages\n    \"\"\"\n    messages = random_messages()\n\n    # iterate through each of the message\n    messages_list = []\n    for handle, message in messages:\n        messages_list.append(html.P([\n            message,\n            html.Br(),\n            \"- \" + handle\n        ]))\n\n    return messages_list\n\nThis function first calls random_messages() to grab some messages and display them using a loop. This function is triggered when the “update” button is pressed, as specified in the callback."
  },
  {
    "objectID": "posts/Web Development with Dash/index.html#running-the-web-app",
    "href": "posts/Web Development with Dash/index.html#running-the-web-app",
    "title": "Building a Message Bank Web App with Dash by Plotly",
    "section": "",
    "text": "Now that we’ve set up all the features and layout of our website, let’s run the app and test its functionality to ensure everything works as expected.\n\nif __name__ == \"__main__\":\n    app.run_server(port=8050, debug=True)\n\n\n        \n        \n\n\nFirst, let’s test the submission function. It works perfectly, displaying a thank you message after a successful submission. \nNext, let’s check the message display feature. We’ve submitted seven messages, each labeled with keywords such as “one”, “two”, “three”, and so on, to indicate the order of submission. Since we’ve capped the display to a maximum of five messages, clicking the update button will randomly select five messages from the database.\n\n\n\nScrenshot of Message Display\n\n\nGreat! We see that the web app is working as desired. With these functions tested and confirmed, our web app is now ready for deployment! :)"
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "",
    "text": "In this project, we’ll create simulations of two-dimensional heat diffusion using four different numerical methods. By comparing their computational performance, we’ll gain insights into which approach is best suited for various scenarios.\nLet’s begin by importing the necessary libraries:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport inspect\nimport jax\nimport jax.numpy as jnp\nimport time\n\nFor this project, we are using the following parameters: N, which notes the grid size of the simulation, and epsilon, a small positive number that controls the timescale of the approximation.\n\nN = 101\nepsilon = 0.2\n\nWe will set up the initial solution by putting 1 unit of heat at the center of the grid.\n\n# construct initial condition\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-matrix-vector-multiplication",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-matrix-vector-multiplication",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Define the Matrix-Vector Multiplication",
    "text": "Define the Matrix-Vector Multiplication\nFirst, we will write a function called advance_time_matvecmul, that advances the simulation by one timestep via matrix-vector multiplication.\n\nfrom heat_equation import advance_time_matvecmul\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nThis function flattens the 2D grid into a vector, and calculates how heat flows between points using the finite difference matrix A.. After adjusting the result by epsilon, which controls simulation stability, we then add the change to the original heat values and reshape this diffusion state back to the original 2D grid."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#create-the-finite-difference-matrix-a",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#create-the-finite-difference-matrix-a",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Create the Finite Difference Matrix A",
    "text": "Create the Finite Difference Matrix A\nIn the function above, we notice the argument Matrix A, which is the 2D finite difference matrix. It encodes the relationship between each grid point and its neighbors. Here, we will use the function get_A(N), which takes in an argument N that specifies and grid size and returns a corresponding matrix A.\n\nfrom heat_equation import get_A\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"\n    constructs the 2d finite difference matrix for heat diffusion simulation\n    argument: N: grid size (N*N)\n    return: A: the N^2 * N^2 finite difference matrix\n    \"\"\"\n    \n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    \n    # Construct matrix A\n    A = np.diag(diagonals[0])                 # Main diagonal (-4)\n    A += np.diag(diagonals[1], 1)             # Right neighbor\n    A += np.diag(diagonals[2], -1)            # Left neighbor\n    A += np.diag(diagonals[3], N)             # Bottom neighbor\n    A += np.diag(diagonals[4], -N)            # Top neighbor\n    \n    return A\n\n\n\nThis function builds a special matrix that represents how heat flows between nearby points on our grid. Let’s break down its structure:\n\nA main diagonal filled with -4’s, which represents each point on the grid\nFour other diagonals filled with 1’s, which represent the four neighboring points (right, left, bottom, top)\nSpecial adjustments for the boundary conditions (where heat can escape)\n\nThe matrix A connects all grid points together according to the heat equation rules, letting us calculate how heat spreads from each point to its neighbors."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nLet’s run a total of 2700 iterations, and visualize the state of heat diffusion every 300 iterations. We will store each simulation state within a list called simulations and plot them in a 3×3 grid.\n\n# Simulation parameters\niterations = 2700  # Total iterations\ninterval = 300  # Save every 300 iterations\nA = get_A(N)\n\n# Store intermediate solutions\nsimulations = []\nu = u0.copy()  # Working copy of the grid\nstart_time = time.time()\n\n# Run the simulation\nfor i in range(1, iterations + 1):\n    u = advance_time_matvecmul(A, u, epsilon)\n    if i % interval == 0:\n        simulations.append(u.copy())  # Save the current state\n\nend_time = time.time()\nprint(f\"Simulation time: {end_time - start_time} seconds\")\n\n# Visualization\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(simulations):\n        im = ax.imshow(simulations[i], cmap='viridis', origin='lower', extent=[0, N, N, 0])\n        ax.set_title(f\"Iteration {(i+1) * interval}\")\n        ax.axis('on')\nplt.tight_layout()\nplt.show()\n\nSimulation time: 36.653226137161255 seconds\n\n\n\n\n\n\n\n\n\nNotice that this simulation took some time to compile – 36.65 seconds. While we can use the underlying optimized matrix multiplication routine from BLAS (basic linear algebra subprograms), it is not particularly effective here, as matrix A a lot of zero elements, wasting time for computation. Will other methods generate visualizations faster than matrix multiplication?"
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#create-the-sparse-matrix-a",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#create-the-sparse-matrix-a",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Create the Sparse Matrix A",
    "text": "Create the Sparse Matrix A\nLet’s define the function get_sparse_A(N), a function that returns A_sp_matrix, the matrix A in a sparse format, given the grid size N as the input. Since most of A’s elements are zero, we’ll store it in BCOO (Batched Coordinate) format, a sparse matrix format supported by JAX.\n\nfrom heat_equation import get_sparse_A\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"\n    args:\n        N: grid size (N*N)\n    return:\n        A_sp_matrix: matrix A in sparse (BCOO) format\n    \"\"\"\n    n = N * N\n    rows, cols, values = [], [], []\n\n    # Main diagonal: -4 for each point\n    for i in range(n):\n        rows.append(i)\n        cols.append(i)\n        values.append(-4.0)\n\n    # Right neighbors\n    for i in range(n - 1):\n        if (i + 1) % N != 0:  # Skip right boundary\n            rows.append(i)\n            cols.append(i + 1)\n            values.append(1.0)\n\n    # Left neighbors\n    for i in range(1, n):\n        if i % N != 0:  # Skip left boundary\n            rows.append(i)\n            cols.append(i - 1)\n            values.append(1.0)\n\n    # Bottom neighbors\n    for i in range(n - N):\n        rows.append(i)\n        cols.append(i + N)\n        values.append(1.0)\n\n    # Top neighbors\n    for i in range(N, n):\n        rows.append(i)\n        cols.append(i - N)\n        values.append(1.0)\n\n    # Convert to BCOO format\n    indices = np.column_stack((rows, cols))\n    A_sp_matrix = sparse.BCOO((np.array(values), indices), shape=(n, n))\n    return A_sp_matrix\n\n\n\nHere, we create lists for row indices (rows), column indices (cols), and values (values). Then fill these lists with the non-zero elements of A: the main diagonal has -4, and the neighboring diagonals have 1. Finally, we convert these lists into a sparse matrix in BCOO format using sparse.BCOO."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-sparse-matrix-vector-multiplication",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-sparse-matrix-vector-multiplication",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Define the Sparse Matrix-Vector Multiplication",
    "text": "Define the Sparse Matrix-Vector Multiplication\nNext, we’ll use JAX’s sparse matrix-vector multiplication to update the grid state. To make it fast, we’ll JIT-compile the function.\n\nfrom heat_equation import advance_time_matvecmul_sparse\nprint(inspect.getsource(advance_time_matvecmul_sparse))\n\n@jax.jit\ndef advance_time_matvecmul_sparse(A_sp, u_flat, epsilon):\n    \"\"\"\n    args:\n        A_sp: the sparse 2d finite difference matrix\n        u_flat: the flattened NxN grid state at timestep k\n        epsilon: stability constant\n    returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    return u_flat + epsilon * (A_sp @ u_flat)\n\n\n\nThe function takes the sparse matrix A_sp, the flattened grid u_flat, and the stability constant epsilon. It computes the new grid state using sparse matrix-vector multiplication, and the result is scaled by epsilon and added to the current state, returning the grid state at timestep k+1."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-1",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-1",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nNow, let’s run the simulation for 2700 iterations, saving the grid state every 300 steps.\n\n# Get sparse matrix A\nA_sp = get_sparse_A(N)\n\n# Store intermediate solutions\nsimulations = []\nu_flat = u0.flatten()  # Flatten the grid for sparse multiplication\nstart_time = time.time()\n\n# Run the simulation\nfor i in range(1, iterations + 1):\n    u_flat = advance_time_matvecmul_sparse(A_sp, u_flat, epsilon)\n    if i % interval == 0:\n        simulations.append(u_flat.reshape((N, N)))  # Save the current state\n\nend_time = time.time()\nprint(f\"Simulation time (sparse): {end_time - start_time} seconds\")\n\n# Generate visualization\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(simulations):\n        im = ax.imshow(simulations[i], cmap='viridis', origin='lower', extent=[0, N, N, 0])\n        ax.set_title(f\"Iteration {(i+1) * interval}\")\n        ax.axis('on')\nplt.tight_layout()\nplt.show()\n\nSimulation time (sparse): 0.5841178894042969 seconds\n\n\n\n\n\n\n\n\n\nLooking at the time taken for compiling these visualizations, we notice that this only takes 0.58 seconds, which is much faster than the matrix multiplication method."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-advance-function",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-advance-function",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Define the Advance Function",
    "text": "Define the Advance Function\nThe function advance_time_numpy will update the grid state using vectorized operations. We’ll pad the grid with zeros to handle boundary conditions and use np.roll() to compute the finite differences.\n\nfrom heat_equation import advance_time_numpy\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"\n    Advances the solution by one timestep using vectorized array operations.\n    \n    Args:\n        u: N x N grid state at timestep k.\n        epsilon: Stability constant.\n        \n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad the grid with zeros to handle boundaries\n    padded = np.pad(u, 1, mode='constant', constant_values=0)\n    \n    # Compute the finite differences using np.roll()\n    u_new = u + epsilon * (\n        np.roll(padded, 1, axis=0)[1:-1, 1:-1] +  # Top neighbor\n        np.roll(padded, -1, axis=0)[1:-1, 1:-1] +  # Bottom neighbor\n        np.roll(padded, 1, axis=1)[1:-1, 1:-1] +   # Left neighbor\n        np.roll(padded, -1, axis=1)[1:-1, 1:-1] -  # Right neighbor\n        4 * u                                       # Center\n    )\n    return u_new\n\n\n\nWe pad the grid u with zeros to create a (N+2) x (N+2) array. This handles the boundary conditions (no heat flow across boundaries). Then, we use np.roll() to shift the grid in four directions (top, bottom, left, right) and compute the finite differences. The new grid state u_new is updated using the 5-point stencil formula"
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-2",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-2",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nNow let’s run the simulation again using the same parameters, visualizing every 300 steps in 2700 iterations.\n\n# Store intermediate solutions\nsimulations = []\nu = u0.copy()  # Working copy of the grid\nstart_time = time.time()\n\n# Run the simulation\nfor i in range(1, iterations + 1):\n    u = advance_time_numpy(u, epsilon)\n    if i % interval == 0:\n        simulations.append(u.copy())  # Save the current state\n\nend_time = time.time()\nprint(f\"Simulation time (NumPy): {end_time - start_time} seconds\")\n\n# Generate visualization\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(simulations):\n        im = ax.imshow(simulations[i], cmap='viridis', origin='lower', extent=[0, N, N, 0])\n        ax.set_title(f\"Iteration {(i+1) * interval}\")\n        ax.axis('on')\nplt.tight_layout()\nplt.show()\n\nSimulation time (NumPy): 0.20039105415344238 seconds\n\n\n\n\n\n\n\n\n\nComparing to the previous two methods, we see that the numpy method generates visualization much faster than the first method, and also a little faster than the second method. Using numpy is simple and effective for simulating heat diffusion."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-advance-function-1",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#define-the-advance-function-1",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Define the Advance Function",
    "text": "Define the Advance Function\nWe’ll use JAX’s jnp.roll() to compute finite differences, just like np.roll() in NumPy. However, JAX does not support in-place updates, so we’ll avoid index assignments.\n\nfrom heat_equation import advance_time_jax\nprint(inspect.getsource(advance_time_jax))\n\n@jax.jit  # JIT-compile this function for performance\ndef advance_time_jax(u, epsilon):\n    \"\"\"\n    Advances the solution by one timestep using JAX and JIT compilation.\n    \n    Args:\n        u: N x N grid state at timestep k (as a JAX array).\n        epsilon: Stability constant.\n        \n    Returns:\n        N x N grid state at timestep k+1.\n    \"\"\"\n    # Pad the grid with zeros to handle boundaries\n    padded = jnp.pad(u, 1, mode='constant', constant_values=0)\n    \n    # Compute the finite differences using jnp.roll()\n    u_new = u + epsilon * (\n        jnp.roll(padded, 1, axis=0)[1:-1, 1:-1] +  # Top neighbor\n        jnp.roll(padded, -1, axis=0)[1:-1, 1:-1] +  # Bottom neighbor\n        jnp.roll(padded, 1, axis=1)[1:-1, 1:-1] +   # Left neighbor\n        jnp.roll(padded, -1, axis=1)[1:-1, 1:-1] -  # Right neighbor\n        4 * u                                        # Center\n    )\n    return u_new\n\n\n\nIn this function, we pad the grid u with zeros to create a (N+2) x (N+2) array, handling boundary conditions. Then, we use jnp.roll() to shift the grid in four directions (top, bottom, left, right) and compute the finite differences. Therefore, the function is JIT-compiled using @jax.jit for high performance."
  },
  {
    "objectID": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-3",
    "href": "posts/Simulating Heat Diffusion with JAX/index.html#running-the-simulation-3",
    "title": "Simulating Heat Diffusion with Matrix Multiplication, NumPy, and JAX",
    "section": "Running the Simulation",
    "text": "Running the Simulation\nWe will now run the simulation again. To take advantage of JIT compilation, we’ll first run the simulation for a small number of iterations to compile the function, and then run it again for the full 2700 iterations.\n\n# Store intermediate solutions\nsimulations = []  # Save the initial state\nu = u0  # Working copy of the grid\n\n# Warm-up run (small number of iterations to compile the function)\nfor _ in range(10):\n    u = advance_time_jax(u, epsilon)\n\n# Main simulation\nstart_time = time.time()\nfor i in range(1, iterations + 1):\n    u = advance_time_jax(u, epsilon)\n    if i % interval == 0:\n        simulations.append(np.array(u))  # Save the current state\n\nend_time = time.time()\nprint(f\"Simulation time (JAX): {end_time - start_time} seconds\")\n\n# Generate visualization\nfig, axes = plt.subplots(3, 3, figsize=(10, 10))\nfor i, ax in enumerate(axes.flat):\n    if i &lt; len(simulations):\n        im = ax.imshow(simulations[i], cmap='viridis', origin='lower', extent=[0, N, N, 0])\n        ax.set_title(f\"Iteration {(i+1) * interval}\")\n        ax.axis('on')\nplt.tight_layout()\nplt.show()\n\nSimulation time (JAX): 0.05159711837768555 seconds\n\n\n\n\n\n\n\n\n\nWow! We see that this method only takes 0.05 seconds, which is even faster than 0.1 seconds! Compared to the 30+ seconds compilation time in the first method, this approach demonstrates a significant improvement in computational performance. By leveraging JAX’s JIT compilation and vectorized operations, we’ve achieved a highly efficient simulation."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "",
    "text": "Have you ever wondered how computers tell apart images showing different categories in the “verify if you are human” questions? In this blog post, we’ll explore image classification using Keras and TensorFlow datasets. We’ll build a system that can distinguish between pictures of cats and dogs – similar to how these verification systems might identify cars, crosswalks, or traffic lights."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#loading-packages-and-obtaining-data",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#loading-packages-and-obtaining-data",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Loading Packages and Obtaining Data",
    "text": "Loading Packages and Obtaining Data\nLet’s import the necessary libraries for our project:\n\nimport os\nimport keras\nfrom keras import utils\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nLet’s first load the dataset. We’ll be using the cats_vs_dogs dataset from Kaggle, which contains labeled images of cats and dogs. We’ll split the dataset into training, validation, and test sets:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\nThe dataset contains images of different sizes, which is problematic for neural networks that expect inputs of consistent dimensions. Let’s resize all images to a fixed size of 150x150 pixels:\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nTo ensure efficient training, we’ll optimize our data pipeline:\n\nfrom tensorflow import data as tf_data\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#understanding-the-data-set",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#understanding-the-data-set",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Understanding the Data Set",
    "text": "Understanding the Data Set\nBefore training a model, it’s important to understand the dataset. Let’s visualize some images to get a sense of what we’re working with. We’ll create a function to display three random cat images and three random dog images:\n\ndef visualize_cats_and_dogs(dataset):\n    cat_images = []\n    dog_images = []\n\n    # retrive 3 images for cats and dogs each\n    for images, labels in dataset.take(1): # take 1 batch\n        for image, label in zip(images, labels):\n            if label == 0 and len(cat_images) &lt; 3:\n                cat_images.append(image.numpy())\n            elif label == 1 and len(dog_images) &lt; 3:\n                dog_images.append(image.numpy())\n            if len(cat_images) == 3 and len(dog_images) == 3:\n                break\n\n    plt.figure(figsize=(10, 5))\n    for i in range(3):\n        plt.subplot(2, 3, i + 1)\n        plt.imshow(cat_images[i] / 255.0)\n        plt.title(\"Cat\")\n        plt.axis(\"off\")\n\n        plt.subplot(2, 3, i + 4)\n        plt.imshow(dog_images[i] / 255.0)\n        plt.title(\"Dog\")\n        plt.axis(\"off\")\n\n    plt.show()\n\nvisualize_cats_and_dogs(train_ds)\n\n\n\n\n\n\n\n\nNext, it’s also important for us to know the distribution of labels in the dataset. This helps us establish a baseline for our model, which is the model tat always guesses the most frequent label. We’ll treat this as the benchmark for improvement.\nLet’s compute the number of cat and dog images in the training set:\n\nlabels_iterator = train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    else:\n        dog_count += 1\n\nbaseline_accuracy = max(cat_count, dog_count) / (cat_count + dog_count) * 100\n\nprint(f\"Number of cat images: {cat_count}\")\nprint(f\"Number of dog images: {dog_count}\")\nprint(f\"Baseline accuracy: {baseline_accuracy:.2f}%\")\n\nNumber of cat images: 4637\nNumber of dog images: 4668\nBaseline accuracy: 50.17%\n\n\nWe see that the accuracy for our baseline model is 50.17%, indicating an even distribution of cats and dogs in our dataset.\nFor the next steps, our goal is to build a model that performs significantly better than this baseline."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Training the Model",
    "text": "Training the Model\nLet’s now train our model for 20 epochs.\n\nmodel1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhist1 = model1.fit(train_ds, epochs = 20, validation_data = validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 18s 85ms/step - accuracy: 0.5511 - loss: 65.1645 - val_accuracy: 0.5989 - val_loss: 0.6488\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.6032 - loss: 0.6564 - val_accuracy: 0.5675 - val_loss: 0.6751\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.5822 - loss: 0.6688 - val_accuracy: 0.5469 - val_loss: 0.6793\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 41ms/step - accuracy: 0.6097 - loss: 0.6501 - val_accuracy: 0.5615 - val_loss: 0.7137\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 42ms/step - accuracy: 0.6397 - loss: 0.6075 - val_accuracy: 0.5353 - val_loss: 0.7725\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.6781 - loss: 0.5726 - val_accuracy: 0.5817 - val_loss: 0.7848\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.7060 - loss: 0.5297 - val_accuracy: 0.5899 - val_loss: 0.8211\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.7358 - loss: 0.5205 - val_accuracy: 0.5950 - val_loss: 0.9273\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.7958 - loss: 0.4139 - val_accuracy: 0.5623 - val_loss: 0.7758\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.8227 - loss: 0.3710 - val_accuracy: 0.5954 - val_loss: 0.8698\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 42ms/step - accuracy: 0.8417 - loss: 0.3473 - val_accuracy: 0.5946 - val_loss: 0.9578\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.8872 - loss: 0.2596 - val_accuracy: 0.6088 - val_loss: 1.0560\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.9114 - loss: 0.2232 - val_accuracy: 0.6053 - val_loss: 1.1473\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 44ms/step - accuracy: 0.9273 - loss: 0.1741 - val_accuracy: 0.6148 - val_loss: 1.2517\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.9358 - loss: 0.1688 - val_accuracy: 0.6075 - val_loss: 1.3676\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.9412 - loss: 0.1495 - val_accuracy: 0.6113 - val_loss: 1.4226\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.9502 - loss: 0.1413 - val_accuracy: 0.6113 - val_loss: 1.5977\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 42ms/step - accuracy: 0.9544 - loss: 0.1380 - val_accuracy: 0.5959 - val_loss: 1.7073\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 42ms/step - accuracy: 0.9444 - loss: 0.1737 - val_accuracy: 0.5985 - val_loss: 1.7170\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 43ms/step - accuracy: 0.9446 - loss: 0.1503 - val_accuracy: 0.6079 - val_loss: 1.8725"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Visualizing Model Accuracy",
    "text": "Visualizing Model Accuracy\nNow, let’s visualize our model training results to better understand its accuracy. We’ll plot the training and validation accuracy to evaluate the model’s performance.\n\n# Plot training and validation accuracy\ndef visualize_model_accuracy(history):\n  plt.plot(history.history['accuracy'], label='Training Accuracy')\n  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.title('Training and Validation Accuracy')\n  plt.legend()\n  plt.show()\n\nvisualize_model_accuracy(hist1)\n\n\n\n\n\n\n\n\nDuring training, the validation accuracy of the model stablizes between 55% to 60% during training. It is slightly better than the baseline model by 5%.\nHowever, overfitting is observed because the training accuracy keeps increasing while the validation accuracy stabilizes at a value well below it. This indicates that we have overfitted our model on the training data set such that it does not generalizes too well to the validation set."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#adding-data-augmentation-layers",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#adding-data-augmentation-layers",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Adding Data Augmentation Layers",
    "text": "Adding Data Augmentation Layers\nWe’ll use 2 argumentation layers: - RandomFlip: Randomly flips images horizontally or vertically. - RandomRotation: Randomly rotates images by a specified angle.\nLet’s visualize the effect of these transformations on a sample image:\n\n# Load a sample image\nfor images, labels in train_ds.take(1):\n    sample_image = images[0].numpy()\n\n# Define augmentation layers\nflip_layer = layers.RandomFlip(\"horizontal_and_vertical\")\nrotate_layer = layers.RandomRotation(0.15)  # Rotate by up to 15%\n\n# Apply augmentations\nflipped_images = [flip_layer(sample_image) for _ in range(3)]\nrotated_images = [rotate_layer(sample_image) for _ in range(3)]\n\n# Plot the original, flipped, and rotated images\nplt.figure(figsize=(10, 7))\n\n# Row 1: Original and flipped images\nplt.subplot(2, 3, 1)\nplt.imshow(sample_image / 255.0)\nplt.title(\"Original\")\nplt.axis(\"off\")\n\nfor i, img in enumerate(flipped_images):\n    plt.subplot(2, 3, i + 2)\n    plt.imshow(img / 255.0)\n    plt.title(f\"Flipped {i + 1}\")\n    plt.axis(\"off\")\n\n# Row 2: Rotated images\nfor i, img in enumerate(rotated_images):\n    plt.subplot(2, 3, i + 4)\n    plt.imshow(img / 255.0)\n    plt.title(f\"Rotated {i + 1}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-1",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-1",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Training the Model",
    "text": "Training the Model\nNow, let’s create model 2 which includes the data augmentation layers. The architecture will be similar to model1, but with augmentation layers added at the beginning:\n\nmodel2 = models.Sequential([\n    layers.Input(shape=(150, 150, 3)),\n\n    # Data augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.15),\n\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    # Fully connected layers\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')  # Binary classification\n])\n\nmodel2.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_1 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_1 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_3 (Conv2D)                    │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_3 (MaxPooling2D)       │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_4 (Conv2D)                    │ (None, 72, 72, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_4 (MaxPooling2D)       │ (None, 36, 36, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_5 (Conv2D)                    │ (None, 34, 34, 128)         │          73,856 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_5 (MaxPooling2D)       │ (None, 17, 17, 128)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_6 (Conv2D)                    │ (None, 15, 15, 256)         │         295,168 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_6 (MaxPooling2D)       │ (None, 7, 7, 256)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_1 (Flatten)                  │ (None, 12544)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (Dense)                      │ (None, 256)                 │       3,211,520 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 256)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (Dense)                      │ (None, 1)                   │             257 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 3,600,193 (13.73 MB)\n\n\n\n Trainable params: 3,600,193 (13.73 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nNow, let’s train the model again for 20 epochs.\n\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhist2 = model2.fit(train_ds, epochs = 20, validation_data = validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 49ms/step - accuracy: 0.7088 - loss: 0.5668 - val_accuracy: 0.7373 - val_loss: 0.5510\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 47ms/step - accuracy: 0.7267 - loss: 0.5535 - val_accuracy: 0.7494 - val_loss: 0.5028\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7331 - loss: 0.5422 - val_accuracy: 0.7485 - val_loss: 0.5093\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.7326 - loss: 0.5315 - val_accuracy: 0.7403 - val_loss: 0.5217\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 47ms/step - accuracy: 0.7527 - loss: 0.5178 - val_accuracy: 0.7730 - val_loss: 0.4758\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7675 - loss: 0.5017 - val_accuracy: 0.7592 - val_loss: 0.4877\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7662 - loss: 0.5031 - val_accuracy: 0.7635 - val_loss: 0.4870\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 47ms/step - accuracy: 0.7670 - loss: 0.4931 - val_accuracy: 0.7747 - val_loss: 0.4678\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7771 - loss: 0.4925 - val_accuracy: 0.7928 - val_loss: 0.4455\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.7762 - loss: 0.4781 - val_accuracy: 0.7773 - val_loss: 0.4723\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7720 - loss: 0.4839 - val_accuracy: 0.7898 - val_loss: 0.4447\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.7846 - loss: 0.4693 - val_accuracy: 0.7975 - val_loss: 0.4438\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.7870 - loss: 0.4654 - val_accuracy: 0.7842 - val_loss: 0.4636\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.7865 - loss: 0.4564 - val_accuracy: 0.8078 - val_loss: 0.4485\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.7933 - loss: 0.4529 - val_accuracy: 0.8147 - val_loss: 0.4142\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.8014 - loss: 0.4360 - val_accuracy: 0.8065 - val_loss: 0.4300\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8030 - loss: 0.4348 - val_accuracy: 0.7885 - val_loss: 0.4772\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 49ms/step - accuracy: 0.7939 - loss: 0.4473 - val_accuracy: 0.8040 - val_loss: 0.4826\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 47ms/step - accuracy: 0.8017 - loss: 0.4253 - val_accuracy: 0.8014 - val_loss: 0.4467\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8132 - loss: 0.4225 - val_accuracy: 0.7971 - val_loss: 0.4436"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-1",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-1",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Visualizing Model Accuracy",
    "text": "Visualizing Model Accuracy\n\nvisualize_model_accuracy(hist2)\n\n\n\n\n\n\n\n\nDuring training, the validation accuracy of the model keeps increasing and stablizes between 75% and 80%, which is about 20% better than model 1. There is not so much overfitting as the trends of the training and validation accuracy align very well."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-2",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-2",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Training the Model",
    "text": "Training the Model\n\nmodel3 = models.Sequential([\n    layers.Input(shape=(150, 150, 3)),\n\n    # Preprocessing layer\n    preprocess_layer,\n\n    # Data augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.15),\n\n    # Convolutional layers\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(256, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n\n    # Fully connected layers\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')  # Binary classification\n])\n\nmodel3.summary()\n\nModel: \"sequential_2\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_2 (Functional)            │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_2 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_2 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_7 (Conv2D)                    │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_7 (MaxPooling2D)       │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_8 (Conv2D)                    │ (None, 72, 72, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_8 (MaxPooling2D)       │ (None, 36, 36, 64)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_9 (Conv2D)                    │ (None, 34, 34, 128)         │          73,856 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_9 (MaxPooling2D)       │ (None, 17, 17, 128)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_10 (Conv2D)                   │ (None, 15, 15, 256)         │         295,168 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_10 (MaxPooling2D)      │ (None, 7, 7, 256)           │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_2 (Flatten)                  │ (None, 12544)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (Dense)                      │ (None, 256)                 │       3,211,520 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (Dropout)                  │ (None, 256)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (Dense)                      │ (None, 1)                   │             257 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 3,600,193 (13.73 MB)\n\n\n\n Trainable params: 3,600,193 (13.73 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhist3 = model3.fit(train_ds, epochs = 20, validation_data = validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 50ms/step - accuracy: 0.8555 - loss: 0.3335 - val_accuracy: 0.8530 - val_loss: 0.3495\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 47ms/step - accuracy: 0.8675 - loss: 0.3000 - val_accuracy: 0.8478 - val_loss: 0.3806\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step - accuracy: 0.8649 - loss: 0.3182 - val_accuracy: 0.8474 - val_loss: 0.3837\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8746 - loss: 0.2924 - val_accuracy: 0.8547 - val_loss: 0.3642\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8776 - loss: 0.2864 - val_accuracy: 0.8525 - val_loss: 0.3940\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8765 - loss: 0.2887 - val_accuracy: 0.8504 - val_loss: 0.3628\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8857 - loss: 0.2737 - val_accuracy: 0.8654 - val_loss: 0.3462\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.8774 - loss: 0.2826 - val_accuracy: 0.8659 - val_loss: 0.3484\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.8796 - loss: 0.2770 - val_accuracy: 0.8637 - val_loss: 0.3447\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 49ms/step - accuracy: 0.8896 - loss: 0.2610 - val_accuracy: 0.8676 - val_loss: 0.3273\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step - accuracy: 0.8943 - loss: 0.2545 - val_accuracy: 0.8826 - val_loss: 0.3035\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8974 - loss: 0.2362 - val_accuracy: 0.8693 - val_loss: 0.3355\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.8984 - loss: 0.2413 - val_accuracy: 0.8624 - val_loss: 0.3550\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.8960 - loss: 0.2452 - val_accuracy: 0.8629 - val_loss: 0.3429\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.8976 - loss: 0.2393 - val_accuracy: 0.8745 - val_loss: 0.3270\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 48ms/step - accuracy: 0.9103 - loss: 0.2282 - val_accuracy: 0.8753 - val_loss: 0.3380\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.9037 - loss: 0.2285 - val_accuracy: 0.8740 - val_loss: 0.3458\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 49ms/step - accuracy: 0.9065 - loss: 0.2293 - val_accuracy: 0.8672 - val_loss: 0.3583\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step - accuracy: 0.9162 - loss: 0.2142 - val_accuracy: 0.8633 - val_loss: 0.3841\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 48ms/step - accuracy: 0.9105 - loss: 0.2225 - val_accuracy: 0.8758 - val_loss: 0.3436"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-2",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-2",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Visualizing Model Accuracy",
    "text": "Visualizing Model Accuracy\n\nvisualize_model_accuracy(hist3)\n\n\n\n\n\n\n\n\nDuring training, the validation accuracy of the model stablizes between 85% and 88%, which is about 30% better than model 1. There is very little evidence of overfitting - the training accuracy is a little (about 0.02%) higher than the validation accuracy."
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#transfer-learning-model",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#transfer-learning-model",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Transfer Learning Model",
    "text": "Transfer Learning Model\nNow, we’ll leverage transfer learning to build a highly accurate model for classifying cats and dogs. Transfer learning allows us to use a pre-trained model (trained on a large dataset like ImageNet) as a starting point for our task. This approach is especially useful when working with limited data, as it enables us to benefit from the features learned by the pre-trained model.\nWe’ll use MobileNetV3Large, a pre-trained model, as the base for our new model.\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n12683000/12683000 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-3",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#training-the-model-3",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Training the Model",
    "text": "Training the Model\nNow, let’s build model4 using data augmentation layers from previous models, MobileNetV3Large as the base model, and additional layers for classification.\n\nmodel4 = models.Sequential([\n    layers.Input(shape=(150, 150, 3)),\n\n    # Data augmentation layers\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.15),\n\n    # Base model (MobileNetV3Large)\n    base_model_layer,\n\n    # Additional layers\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation='sigmoid')  # Binary classification\n])\n\nmodel4.summary()\n\nModel: \"sequential_3\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_3 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_3 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_4 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (Dropout)                  │ (None, 960)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_6 (Dense)                      │ (None, 1)                   │             961 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,997,313 (11.43 MB)\n\n\n\n Trainable params: 961 (3.75 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nIn the summary of model 4, we notice that there are 2,996,352 non-trainable parameters, which are hidden in the base_model_layer. Therefore, we are only training 961 parameters here.\n\nmodel4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhist4 = model4.fit(train_ds, epochs = 20, validation_data = validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 18s 71ms/step - accuracy: 0.7008 - loss: 2.2544 - val_accuracy: 0.9518 - val_loss: 0.2481\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 60ms/step - accuracy: 0.8775 - loss: 0.7049 - val_accuracy: 0.9630 - val_loss: 0.1839\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.8905 - loss: 0.5860 - val_accuracy: 0.9656 - val_loss: 0.1699\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.8991 - loss: 0.5048 - val_accuracy: 0.9678 - val_loss: 0.1544\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9126 - loss: 0.3838 - val_accuracy: 0.9673 - val_loss: 0.1413\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9134 - loss: 0.3689 - val_accuracy: 0.9695 - val_loss: 0.1227\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 64ms/step - accuracy: 0.9145 - loss: 0.3188 - val_accuracy: 0.9652 - val_loss: 0.1169\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 54ms/step - accuracy: 0.9151 - loss: 0.2863 - val_accuracy: 0.9669 - val_loss: 0.1055\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9159 - loss: 0.2685 - val_accuracy: 0.9660 - val_loss: 0.1049\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9205 - loss: 0.2463 - val_accuracy: 0.9630 - val_loss: 0.1100\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9182 - loss: 0.2563 - val_accuracy: 0.9673 - val_loss: 0.0959\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 57ms/step - accuracy: 0.9146 - loss: 0.2586 - val_accuracy: 0.9669 - val_loss: 0.0896\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 62ms/step - accuracy: 0.9098 - loss: 0.2573 - val_accuracy: 0.9699 - val_loss: 0.0879\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9239 - loss: 0.2115 - val_accuracy: 0.9682 - val_loss: 0.0813\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 56ms/step - accuracy: 0.9249 - loss: 0.2047 - val_accuracy: 0.9630 - val_loss: 0.0978\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9166 - loss: 0.2297 - val_accuracy: 0.9669 - val_loss: 0.0952\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 11s 62ms/step - accuracy: 0.9162 - loss: 0.2338 - val_accuracy: 0.9518 - val_loss: 0.1385\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 54ms/step - accuracy: 0.9123 - loss: 0.2417 - val_accuracy: 0.9587 - val_loss: 0.1172\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9203 - loss: 0.2376 - val_accuracy: 0.9639 - val_loss: 0.1086\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 55ms/step - accuracy: 0.9116 - loss: 0.2472 - val_accuracy: 0.9708 - val_loss: 0.0812"
  },
  {
    "objectID": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-3",
    "href": "posts/Image Classification with Keras and Tensorflow/index.html#visualizing-model-accuracy-3",
    "title": "Classifying Images of Cats and Dogs in Keras",
    "section": "Visualizing Model Accuracy",
    "text": "Visualizing Model Accuracy\n\nvisualize_model_accuracy(hist4)\n\n\n\n\n\n\n\n\nDuring training, the validation accuracy of the model stablizes above 95%. This is 30% better than model 1! There is little overfitting as the accuracy of the validation set exceeds that of the training set."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html",
    "href": "posts/Fake News Classification using Keras/index.html",
    "title": "Fake News Classification using Keras",
    "section": "",
    "text": "In earlier tutorials, we have already worked with image classification. In this tutorial, we are going to work with another common form of data – text. We will be developing and evaluating text classifiers – specifically fake news classifiers – using Keras."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#set-up-the-spider",
    "href": "posts/Fake News Classification using Keras/index.html#set-up-the-spider",
    "title": "Create A Movie Recommender through Web Scraping",
    "section": "",
    "text": "For this tutorial, we will use TMDB (https://www.themoviedb.org/) to access comprehensive movie data, including cast and crew information.\nFirst, let’s create a file in the spiders directory called tmdb_spider.py. We will add the following lines to the file:\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\nThis creates a spider named tmdb_spider that will start scraping from a specified TMDB movie page. The subdir parameter allows us to specify which movie’s page to begin scraping.\nThis spider follows a three-stage scraping process. It begins at a movie’s main page, and then navigates to the Full Cast & Crew page. Once it reaches the cast page, it will access the individual actor pages. For each actor, it then collects a list of other movies or TV shows they have appeared in.\nWe’re going to create three parsing methods for each stage of the scraping.\n\n\nThe first parsing method we write in the spider will assume that we start on a movie page, and then navigate to the Full Cast & Crew Page. We write it as the follows:\n\ndef parse(self, response):\n        \"\"\"\n        parse the movie page and navigate to the full cast & crew page\n        arguments: the response object containing the movie page HTML\n        yields a request to follow the cast page URL, with parse_full_credits as callback\n        \"\"\"\n        \n        cast_page_link = response.css('.new_button a::attr(href)').get()\n        yield response.follow(cast_page_link, callback = self.parse_full_credits) # follow URL of the cast page\n\nThis method appends “/cast” to the movie page URL to access the Cast & Crew page. When called, it yields a new request that Scrapy will follow, using parse_full_credits as the callback method to process the resulting page.\n\n\n\nThe second parsing method, parse_full_credits, will access the individual pages of actors (not including crew members) in the specified movie.\n\ndef parse_full_credits(self, response):\n    \"\"\"\n    parses the full cast and crew page and navigate to actors' individual pages\n    arguments: the response object containing the cast & crew page HTML\n    yields: request to follow each actors' page URL, with parse_actor_page as callback\n    \"\"\"\n    \n    # access the relative URLs for actors' individual pages\n    actor_pages = response.css('ol.people.credits:not(.crew) li &gt; a::attr(href)').getall()\n\n    # yield new request for each actor page\n    for actor in actor_pages:\n        yield response.follow(actor, callback = self.parse_actor_page)\n\nIn this method, we first use a CSS selector to find all the relative URLs for actors’ individual pages. We use the symbol ‘&gt;’ to make sure that we only select a elements that are direct children of li elements. It then iterates through each of these URLs and create a new request for each actor’s page, which will then be processed by the parse_actor_page method that we will look into next.\n\n\n\nNow we have reached the third stage of scraping! We will write the third parsing method, parse_actor_page. This method collects each actor’s name and their complete list of acting roles, generating a dictionary for each unique movie or TV show they’ve appeared in.\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    parse the actor page and return a dictionary for actor name and movie/tv names\n    argument: the response object containing the actor's page HTML\n    yield a dictionary of the form {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name},\n    which records a list of unique movie/tv names where the actor has acted\n    \"\"\"\n    # extract actor name\n    actor_name = response.css('h2.title a::text').get()\n\n    # extract the categories the actor have worked in (acting, production, ...)\n    categories = response.css('div.credits_list h3::text').getall()\n\n    # extract the table of credits for all categories\n    all_titles = response.css('div.credits_list table.credits')\n\n    for i, category in enumerate(categories):\n        if category == 'Acting': # only check the acting categories\n            titles = all_titles[i].css('table.credit_group tr a.tool:tip bdi::text').getall()\n\n            # get unique titles using set()\n            unique_titles = list(set(titles))\n\n            for title in unique_titles:\n                yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n            break # break when category is no longer \"acting\"\n\nThis method first extracts the actor’s name, the categories the actor have worked in (acting, production…), and the table of credits for all categories. It then checks if the category is “Acting”, and then extract the titles of movies/TV under the acting credits. The function set() is used to get unique titles, and from there, we will yield a dictionary containing both the actor’s name and the movie/TV title.\nThis method will iterate through all actors of the specified movie, so we will get a long list of all the movie/TV titles that each of the actors of the movie have acted in."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#using-the-scraper-for-movie-recommendation",
    "href": "posts/Fake News Classification using Keras/index.html#using-the-scraper-for-movie-recommendation",
    "title": "Create A Movie Recommender through Web Scraping",
    "section": "",
    "text": "One of my favorite movies is Mulholland Drive (2001) directed by David Lynch. It is a masterpiece of surrealist cinema that blends mystery, psychological thriller, and noir elements to create an enigmatic narrative about dreams and reality in Hollywood.\nThe link to this movie page is https://www.themoviedb.org/movie/1018-mulholland-drive, and let’s test our scraper to see if it works well in generating movie recommendations!\nWe will run the following command in the terminal (make sure you are in the same directory as the spider):\nscrapy crawl tmdb_spider -o results.csv -a subdir=1018-mulholland-drive\nThis command will generate a CSV file named results.csv in your directory. The file will contain a comprehensive list of actors from Mulholland Drive and their corresponding filmographies, which we can analyze to identify potential movie recommendations based on cast overlap.\n\n\nLet’s evaluate our scraper’s effectiveness in generating movie recommendations by analyzing the cast connections.\nFirst, let’s compute a sorted list with the top movies and TC shows that share actors with Mulholland Drive. It will have two columns: “move names” and “number of shared actors”.\nLet’s first import necessary libraries and load the data:\n\nimport pandas as pd\nmovie = pd.read_csv(\"TMDB_scraper/TMDB_scraper/spiders/results.csv\")\n\nNext, we will group the movie dataframe by movie name and count the number of unique actors in each of them. We will then sort the dataframe by the number of shared actors in descending order:\n\n# Filter out Mulholland Drive to only get other movies\nother_movies = movie[~movie['movie_or_TV_name'].isin(['Mulholland Dr.', 'Mulholland Drive'])]\n\n# Group by movie name and count unique actors\nmovie_connections = other_movies.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\n\n# Rename columns to match desired format\nmovie_connections.columns = ['movie names', 'number of shared actors']\n\n# Sort by number of shared actors in descending order\nmovie_recommendations = movie_connections.sort_values(by='number of shared actors', ascending=False)\n\nmovie_recommendations\n\n\n\n\n\n\n\n\nmovie names\nnumber of shared actors\n\n\n\n\n273\nCSI: Crime Scene Investigation\n8\n\n\n2037\nTwin Peaks\n8\n\n\n836\nJAG\n7\n\n\n345\nCold Case\n7\n\n\n983\nMacGyver\n6\n\n\n...\n...\n...\n\n\n764\nHome of the Giants\n1\n\n\n763\nHome and Away\n1\n\n\n762\nHome Improvement\n1\n\n\n761\nHome\n1\n\n\n2185\nseaQuest DSV\n1\n\n\n\n\n2186 rows × 2 columns\n\n\n\n\n\n\nLet’s visualize the result we get above using a bar plot. For this visualization, we will look at the top 10 movies recommended by the scraper.\n\n# import necessary libraries\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nfrom plotly import express as px\n\nfig = px.bar(movie_recommendations.head(10),\n            x='number of shared actors',\n            y='movie names',\n            orientation='h',\n            title='Top 10 Movies/TV Shows Recommended by the Scraper')\n\nfig.show()\n\n\n\n\nGreat! Do these recommended movies meet your expectations? Or will there be better ways for us to generate better movie recommendations?"
  },
  {
    "objectID": "posts/chairs/index.html",
    "href": "posts/chairs/index.html",
    "title": "PIC 16B Final Group Submission",
    "section": "",
    "text": "Chairness is a web application designed to generate images of unique and creative chair designs. The application leverages a combination of web-scraped chair images from e-commerce platforms and synthetically generated chair images from 3D models using automated Blender scripts. These images are used to train a diffusion network, which powers the image generation process.\nYou can find the link to our Github repository here: https://github.com/shruaibylsh/chairness\nThis is a flow chart illustrating the structure of our project:"
  },
  {
    "objectID": "posts/chairs/index.html#web-scraping-from-pinterest-using-selenium",
    "href": "posts/chairs/index.html#web-scraping-from-pinterest-using-selenium",
    "title": "PIC 16B Final Group Submission",
    "section": "Web-Scraping from Pinterest using Selenium",
    "text": "Web-Scraping from Pinterest using Selenium\nWe implemented a custom scraper to collect chair images and metadata from Pinterest. The scraper navigates to search results for specified keywords such as “designer chairs” and collects: 1. Pinterest ID 2. Chair name 3. Description 4. Alt text 5. Image URL\nThe implementation focuses on finding pins with the specific CSS selector div[data-test-id=‘pin-with-alt-text’] and then visiting each individual pin page to extract more detailed information.\nThis is a code snippet for our scraper:\n\ndef scrape_pinterest(num_images=500):\n    driver.get(\"https://www.pinterest.com/search/pins/?q=vitra%20chair%20design\")\n    time.sleep(5)  # Wait for page to load\n    \n    pin_data = []\n    scroll_count = 0\n    max_scrolls = 100\n    \n    while len(pin_data) &lt; num_images and scroll_count &lt; max_scrolls:\n        pin_elements = driver.find_elements(By.CSS_SELECTOR, \"div[data-test-id='pin-with-alt-text']\")\n        \n        for pin in pin_elements:\n            if len(pin_data) &gt;= num_images:\n                break\n                \n            try:\n                a_tag = pin.find_element(By.TAG_NAME, \"a\")\n                href = a_tag.get_attribute(\"href\")\n                \n                if \"/pin/\" in href:\n                    pin_id = href.split(\"/pin/\")[1].split(\"/\")[0]\n                    \n                    if not any(p[0] == pin_id for p in pin_data):\n                        alt_text = a_tag.get_attribute(\"aria-label\")\n                        pin_data.append((pin_id, alt_text))\n                        print(f\"Found pin: {pin_id} - {alt_text}\")\n            except Exception as e:\n                print(f\"Error extracting pin data: {e}\")\n        \n        if len(pin_data) &lt; num_images:\n            driver.execute_script(\"window.scrollBy(0, 1000);\")\n            time.sleep(2)\n            scroll_count += 1\n    \n    print(f\"Extracted data for {len(pin_data)} pins\")\n    \n    for idx, (pin_id, alt_text) in enumerate(pin_data):\n        try:\n            pin_url = f\"https://www.pinterest.com/pin/{pin_id}/\"\n            driver.get(pin_url)\n            \n            # Extract name, description, and image URL\n            ...\n            \n            if img_url:\n                img_filename = f\"{pin_id}.jpg\"\n                img_path = os.path.join(image_folder, img_filename)\n                \n                response = requests.get(img_url, headers=headers, timeout=10)\n                if response.status_code == 200:\n                    with open(img_path, \"wb\") as f:\n                        f.write(response.content)\n                    print(f\"Downloaded image: {img_filename}\")\n        except Exception as e:\n            print(f\"Error processing pin {pin_id}: {e}\")\n\nThese are images of our scraped chairs and an example of the csv file we have compiled:"
  },
  {
    "objectID": "posts/chairs/index.html#synthetic-data-generation-using-blender",
    "href": "posts/chairs/index.html#synthetic-data-generation-using-blender",
    "title": "PIC 16B Final Group Submission",
    "section": "Synthetic Data Generation using Blender",
    "text": "Synthetic Data Generation using Blender\nWe then wrote a script that automates the generation of synthetic chair images using Blender, a powerful 3D rendering tool. It imports 3D chair models (e.g., .obj, .fbx, .blend, .gltf) and renders them from multiple angles and elevations to create a diverse dataset. The script sets up a Blender scene with a three-point lighting system, a camera, and a transparent background for high-quality renders. It also supports material variations, allowing for different textures and colors to be applied to the chair models, further enhancing dataset diversity.\nThe script processes each chair model by normalizing its size, centering it in the scene, and applying textures from a specified directory. It then renders the chair from 12 horizontal angles and 3 elevation angles, producing 36 images per model. GPU-accelerated rendering with Cycles ensures efficient and high-quality output. The resulting images are saved in PNG format with transparency, ready for use in training machine learning models.\nHere is a code snippet:\n\ndef setup_scene():\n    \"\"\"Set up the Blender scene with lighting and camera.\"\"\"\n    bpy.ops.object.select_all(action='DESELECT')\n    bpy.ops.object.select_by_type(type='MESH')\n    bpy.ops.object.select_by_type(type='LIGHT')\n    bpy.ops.object.select_by_type(type='CAMERA')\n    bpy.ops.object.delete()\n\n    # Add camera and lights\n    bpy.ops.object.camera_add(location=(0, -3, 1.5), rotation=(math.radians(75), 0, 0))\n    camera = bpy.context.active_object\n    bpy.context.scene.camera = camera\n\n    # Three-point lighting setup\n    bpy.ops.object.light_add(type='AREA', radius=3, location=(3, -2, 3))\n    key_light = bpy.context.active_object\n    key_light.data.energy = 500\n\n    bpy.ops.object.light_add(type='AREA', radius=2, location=(-3, -2, 2))\n    fill_light = bpy.context.active_object\n    fill_light.data.energy = 300\n\n    bpy.ops.object.light_add(type='AREA', radius=2, location=(0, 3, 2))\n    rim_light = bpy.context.active_object\n    rim_light.data.energy = 400\n\n    return camera\n\ndef import_chair_model(filepath):\n    \"\"\"Import and normalize a chair model.\"\"\"\n    bpy.ops.object.select_all(action='DESELECT')\n    bpy.ops.object.select_by_type(type='MESH')\n    bpy.ops.object.delete()\n\n    ext = os.path.splitext(filepath)[1].lower()\n    if ext == '.fbx':\n        bpy.ops.import_scene.fbx(filepath=filepath)\n    elif ext == '.obj':\n        bpy.ops.import_scene.obj(filepath=filepath)\n    else:\n        raise ValueError(f\"Unsupported format: {ext}\")\n\n    chair = bpy.context.active_object\n    chair.scale = (2, 2, 2)\n    bpy.ops.object.transform_apply(scale=True)\n    return chair\n\ndef render_chair_angles(chair, output_path, num_angles=12):\n    \"\"\"Render the chair from multiple angles.\"\"\"\n    camera = bpy.context.scene.camera\n    bpy.ops.object.empty_add(type='PLAIN_AXES', location=(0, 0, chair.dimensions.z / 2))\n    target = bpy.context.active_object\n    constraint = camera.constraints.new(type='TRACK_TO')\n    constraint.target = target\n\n    for angle_idx in range(num_angles):\n        angle = 2 * math.pi * angle_idx / num_angles\n        x = 3 * math.sin(angle)\n        y = 3 * math.cos(angle)\n        camera.location = (x, y, 1.5)\n        bpy.context.scene.render.filepath = os.path.join(output_path, f\"angle_{angle_idx:02d}.png\")\n        bpy.ops.render.render(write_still=True)\n\n    bpy.ops.object.select_all(action='DESELECT')\n    target.select_set(True)\n    bpy.ops.object.delete()\n\nHere are some images we have generated through this script:"
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#importing-data",
    "href": "posts/Fake News Classification using Keras/index.html#importing-data",
    "title": "Fake News Classification using Keras",
    "section": "Importing Data",
    "text": "Importing Data\nLet’s first import the necessary libraries for this project:\n\nimport tensorflow as tf\nimport keras\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\n\nLet’s now load the data for this project:\n\ntrain_url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/refs/heads/main/datasets/fake_news_train.csv\"\ndf = pd.read_csv(train_url)\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nInspecting our data, we see that each row of the data corresponds to an article. The title column gives the title of the article, while the text column gives the full article text. The final column, called fake, is 0 if the article is true and 1 if the article contains fake news."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#dataset-preprocessing",
    "href": "posts/Fake News Classification using Keras/index.html#dataset-preprocessing",
    "title": "Fake News Classification using Keras",
    "section": "Dataset Preprocessing",
    "text": "Dataset Preprocessing\nNext, we will write a function called make_dataset that prepares the data for training. This function will perform the following steps:\n\nConvert text to lowercase: This ensures that the model treats words like “News” and “news” as the same.\nRemove stopwords: Stopwords are common words like “the,” “and,” or “but” that are usually uninformative for text classification tasks. We will remove these to reduce noise in the data.\nConstruct a tf.data.Dataset: The dataset will have two inputs (the title and text) and one output (the fake label).\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Get stopwords\nnltk.download('stopwords')\nenglish_stopwords = set(stopwords.words('english'))\n\ndef make_dataset(dataframe, batch_size=100):\n\n    # Create a copy of the dataframe\n    clean_df = dataframe.copy()\n\n    # Clean title and text columns\n    for column in ['title', 'text']:\n        clean_df[column] = clean_df[column].apply(lambda text: clean_text(text))\n\n    # Create and return the dataset\n    features = {\"title\": clean_df[\"title\"], \"text\": clean_df[\"text\"]}\n    labels = clean_df[\"fake\"]\n\n    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n    dataset = dataset.shuffle(len(clean_df), reshuffle_each_iteration=False)\n    dataset = dataset.batch(batch_size)\n\n    return dataset\n\ndef clean_text(text):\n\n    if not isinstance(text, str):\n        return \"\"\n\n    words = text.lower().split()\n    clean_words = [word for word in words if word not in english_stopwords]\n    return \" \".join(clean_words)\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\nValidation Data\nFor this project, we will use 20% of out data for the validation set.\n\ntotal_samples = len(df)\ntrain_ratio = 0.8  # Use 80% for training\ntrain_samples = int(train_ratio * total_samples)\nval_samples = total_samples - train_samples\n\n# Create and split dataset\ndataset = make_dataset(df)\ntrain_dataset = dataset.take(train_samples // 100)  # Divide by batch size (100)\nval_dataset = dataset.skip(train_samples // 100)\n\n\n\nBase Rate\nThe base rate refers to the accuracy of a model that always predicts the majority class. Calculating the base rate helps us understand the performance of a naive model and provides a benchmark for our classifier.\nLet’s calculate the base rate for our dataset:\n\n# Calculate the proportion of fake news articles in the training set\nbase_rate = df['fake'].mean()\nprint(f\"Base rate: {base_rate:.2f}\")\n\nBase rate: 0.52\n\n\n\n\nText Vectorization\nTo process the text data for our model, we’ll use TensorFlow’s text vectorization layer to vectorize our data:\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                 '[%s]' % re.escape(string.punctuation), '')\n    return no_punctuation\n\n# Create vectorization layers\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500)\n\n# Adapt the vectorization layers to the data\ntitle_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"title\"]))\ntext_vectorize_layer.adapt(train_dataset.map(lambda x, y: x[\"text\"]))"
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#first-model",
    "href": "posts/Fake News Classification using Keras/index.html#first-model",
    "title": "Fake News Classification using Keras",
    "section": "First Model",
    "text": "First Model\nFor the first model, we will only use the article title as an input.\n\ntitle_input = keras.Input(shape=(1,), name=\"title\", dtype=\"string\")\nx = title_vectorize_layer(title_input)\nx = layers.Embedding(size_vocabulary, output_dim=64, name=\"title_embedding\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.5)(x)\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\ntitle_model = keras.Model(inputs={\"title\": title_input}, outputs=output)\ntitle_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nfrom keras.utils import plot_model\nplot_model(title_model, \"title.png\", show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\nLet’s see the performance of our model and visualize the model accuracy:\n\ntitle_history = train_model(title_model, train_dataset, val_dataset)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: {'title': 'title'}\nReceived: inputs={'title': 'Tensor(shape=(None,))', 'text': 'Tensor(shape=(None,))'}\n  warnings.warn(msg)\n\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 8s 19ms/step - accuracy: 0.6073 - loss: 0.6722 - val_accuracy: 0.8934 - val_loss: 0.5729\nEpoch 2/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.8238 - loss: 0.5356 - val_accuracy: 0.9272 - val_loss: 0.4049\nEpoch 3/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.8846 - loss: 0.3875 - val_accuracy: 0.9316 - val_loss: 0.3078\nEpoch 4/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9086 - loss: 0.3013 - val_accuracy: 0.9457 - val_loss: 0.2545\nEpoch 5/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9226 - loss: 0.2548 - val_accuracy: 0.9468 - val_loss: 0.2233\nEpoch 6/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9279 - loss: 0.2264 - val_accuracy: 0.9530 - val_loss: 0.2026\nEpoch 7/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9357 - loss: 0.2042 - val_accuracy: 0.9565 - val_loss: 0.1884\nEpoch 8/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9407 - loss: 0.1903 - val_accuracy: 0.9580 - val_loss: 0.1778\nEpoch 9/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9443 - loss: 0.1798 - val_accuracy: 0.9602 - val_loss: 0.1691\nEpoch 10/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9466 - loss: 0.1677 - val_accuracy: 0.9426 - val_loss: 0.1641\nEpoch 11/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9499 - loss: 0.1618 - val_accuracy: 0.9620 - val_loss: 0.1568\nEpoch 12/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9510 - loss: 0.1547 - val_accuracy: 0.9613 - val_loss: 0.1493\nEpoch 13/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9552 - loss: 0.1483 - val_accuracy: 0.9620 - val_loss: 0.1448\nEpoch 14/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9584 - loss: 0.1443 - val_accuracy: 0.9631 - val_loss: 0.1426\nEpoch 15/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9551 - loss: 0.1399 - val_accuracy: 0.9639 - val_loss: 0.1384\nEpoch 16/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9574 - loss: 0.1357 - val_accuracy: 0.9655 - val_loss: 0.1361\nEpoch 17/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9602 - loss: 0.1323 - val_accuracy: 0.9472 - val_loss: 0.1361\nEpoch 18/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9599 - loss: 0.1298 - val_accuracy: 0.9644 - val_loss: 0.1300\nEpoch 19/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 23ms/step - accuracy: 0.9603 - loss: 0.1271 - val_accuracy: 0.9661 - val_loss: 0.1274\nEpoch 20/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9637 - loss: 0.1204 - val_accuracy: 0.9666 - val_loss: 0.1257\n46/46 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.9655 - loss: 0.1225\nvalidation accuracy: 0.9666\n\n\n\nvisualize_accuracy(title_history)\n\n\n\n\n\n\n\n\nFrom the graph, we observe that the model accuracy stablizes at around 97%."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#second-model",
    "href": "posts/Fake News Classification using Keras/index.html#second-model",
    "title": "Fake News Classification using Keras",
    "section": "Second Model",
    "text": "Second Model\nFor the second model, we will only use the article text as an input.\n\ntext_input = keras.Input(shape=(1,), name=\"text\", dtype=\"string\")\nx = title_vectorize_layer(text_input)\nx = layers.Embedding(size_vocabulary, output_dim=64, name=\"title_embedding\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.5)(x)\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\ntext_model = keras.Model(inputs={\"text\": text_input}, outputs=output)\ntext_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nplot_model(text_model, \"text.png\", show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\nLet’s train this model and visualize its result:\n\ntext_history = train_model(text_model, train_dataset, val_dataset)\n\nEpoch 1/20\n\n\n/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: {'text': 'text'}\nReceived: inputs={'title': 'Tensor(shape=(None,))', 'text': 'Tensor(shape=(None,))'}\n  warnings.warn(msg)\n\n\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.6099 - loss: 0.6729 - val_accuracy: 0.9107 - val_loss: 0.5759\nEpoch 2/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.8201 - loss: 0.5378 - val_accuracy: 0.9151 - val_loss: 0.4069\nEpoch 3/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.8891 - loss: 0.3875 - val_accuracy: 0.9316 - val_loss: 0.3080\nEpoch 4/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9074 - loss: 0.3022 - val_accuracy: 0.9442 - val_loss: 0.2548\nEpoch 5/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9216 - loss: 0.2554 - val_accuracy: 0.9521 - val_loss: 0.2249\nEpoch 6/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9323 - loss: 0.2245 - val_accuracy: 0.9530 - val_loss: 0.2028\nEpoch 7/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9333 - loss: 0.2069 - val_accuracy: 0.9569 - val_loss: 0.1881\nEpoch 8/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9410 - loss: 0.1912 - val_accuracy: 0.9582 - val_loss: 0.1780\nEpoch 9/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 14ms/step - accuracy: 0.9433 - loss: 0.1794 - val_accuracy: 0.9600 - val_loss: 0.1680\nEpoch 10/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9492 - loss: 0.1684 - val_accuracy: 0.9615 - val_loss: 0.1624\nEpoch 11/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9489 - loss: 0.1619 - val_accuracy: 0.9613 - val_loss: 0.1550\nEpoch 12/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9524 - loss: 0.1545 - val_accuracy: 0.9620 - val_loss: 0.1497\nEpoch 13/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9551 - loss: 0.1487 - val_accuracy: 0.9626 - val_loss: 0.1460\nEpoch 14/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9559 - loss: 0.1454 - val_accuracy: 0.9628 - val_loss: 0.1438\nEpoch 15/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9574 - loss: 0.1383 - val_accuracy: 0.9631 - val_loss: 0.1381\nEpoch 16/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9580 - loss: 0.1344 - val_accuracy: 0.9644 - val_loss: 0.1355\nEpoch 17/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9594 - loss: 0.1328 - val_accuracy: 0.9653 - val_loss: 0.1336\nEpoch 18/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9586 - loss: 0.1309 - val_accuracy: 0.9653 - val_loss: 0.1298\nEpoch 19/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9587 - loss: 0.1276 - val_accuracy: 0.9655 - val_loss: 0.1299\nEpoch 20/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9598 - loss: 0.1231 - val_accuracy: 0.9661 - val_loss: 0.1271\n46/46 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.9659 - loss: 0.1233\nvalidation accuracy: 0.9661\n\n\n\nvisualize_accuracy(text_history)\n\n\n\n\n\n\n\n\nWe observe from the graph this time the validation accuracy is more stable than the first model, achieving around 97% accuracy."
  },
  {
    "objectID": "posts/Fake News Classification using Keras/index.html#third-model",
    "href": "posts/Fake News Classification using Keras/index.html#third-model",
    "title": "Fake News Classification using Keras",
    "section": "Third Model",
    "text": "Third Model\nFor the third model, we are going to investigate the combined effect when we have both the article title and the article text as inputs.\n\nt = text_vectorize_layer(title_input)\nx = text_vectorize_layer(text_input)\n\n# Shared embedding layer\nshared_embedding = layers.Embedding(\n    size_vocabulary,\n    output_dim=64,\n    name=\"shared_embedding\"\n)\n\n# Title branch\nt = shared_embedding(t)\nt = layers.Dropout(0.5)(t)\nt = layers.GlobalAveragePooling1D()(t)\nt = layers.Dropout(0.5)(t)\nt = layers.Dense(30, activation=\"relu\")(t)\n\n# Text branch\nx = shared_embedding(x)\nx = layers.Dropout(0.5)(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(30, activation=\"relu\")(x)\n\n# Combine branches\nmixed = layers.concatenate([t, x])\nx = layers.Dropout(0.5)(mixed)\nx = layers.Dense(30, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\noutput = layers.Dense(1, activation=\"sigmoid\")(x)\n\n# Create and compile model\nmixed_model = keras.Model(inputs={\"title\": title_input, \"text\": text_input}, outputs=output)\nmixed_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nplot_model(mixed_model, \"mixed.png\", show_shapes=True, show_layer_names=True)\n\n\n\n\n\n\n\n\nLet’s also train the model and visualize its result:\n\nmixed_history = train_model(mixed_model, train_dataset, val_dataset)\n\nEpoch 1/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9723 - loss: 0.0782 - val_accuracy: 0.9776 - val_loss: 0.0838\nEpoch 2/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9770 - loss: 0.0670 - val_accuracy: 0.9813 - val_loss: 0.0769\nEpoch 3/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9598 - loss: 0.0974 - val_accuracy: 0.9822 - val_loss: 0.0777\nEpoch 4/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9777 - loss: 0.0631 - val_accuracy: 0.9765 - val_loss: 0.0835\nEpoch 5/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9739 - loss: 0.0703 - val_accuracy: 0.9780 - val_loss: 0.0768\nEpoch 6/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 25ms/step - accuracy: 0.9714 - loss: 0.0747 - val_accuracy: 0.9802 - val_loss: 0.0774\nEpoch 7/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9794 - loss: 0.0563 - val_accuracy: 0.9820 - val_loss: 0.0758\nEpoch 8/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 22ms/step - accuracy: 0.9761 - loss: 0.0666 - val_accuracy: 0.9800 - val_loss: 0.0791\nEpoch 9/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 5s 26ms/step - accuracy: 0.9824 - loss: 0.0559 - val_accuracy: 0.9813 - val_loss: 0.0783\nEpoch 10/20\n179/179 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.9711 - loss: 0.0767 - val_accuracy: 0.9813 - val_loss: 0.0772\n46/46 ━━━━━━━━━━━━━━━━━━━━ 1s 10ms/step - accuracy: 0.9819 - loss: 0.0775\nvalidation accuracy: 0.9820\n\n\n\nvisualize_accuracy(mixed_history)\n\n\n\n\n\n\n\n\nFrom the graph, we observe that the model accuracy is also very high, stably maintaining a level of around 98%. As a result, model 3 performs the best out of the three models."
  }
]
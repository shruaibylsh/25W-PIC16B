{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"PIC 16B Final Group Submission\"\n",
        "author: \"Group #5\"\n",
        "date: \"2025-03-21\"\n",
        "categories: [project]\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfI1LFgWQMaZ"
      },
      "source": [
        "# Overview\n",
        "**Chairness** is a web application designed to generate images of unique and creative chair designs. The application leverages a combination of web-scraped chair images from e-commerce platforms and synthetically generated chair images from 3D models using automated Blender scripts. These images are used to train a diffusion network, which powers the image generation process.\n",
        "\n",
        "You can find the link to our Github repository here: https://github.com/shruaibylsh/chairness\n",
        "\n",
        "This is a flow chart illustrating the structure of our project:\n",
        "![flow chart](flow_chart.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs2KqPUZQeNn"
      },
      "source": [
        "# Data Acquisition\n",
        "## Web-Scraping from Pinterest using Selenium\n",
        "We implemented a custom scraper to collect chair images and metadata from Pinterest. The scraper navigates to search results for specified keywords such as \"designer chairs\" and collects:\n",
        "1. Pinterest ID\n",
        "2. Chair name\n",
        "3. Description\n",
        "4. Alt text\n",
        "5. Image URL\n",
        "\n",
        "The implementation focuses on finding pins with the specific CSS selector div[data-test-id='pin-with-alt-text'] and then visiting each individual pin page to extract more detailed information.\n",
        "\n",
        "This is a code snippet for our scraper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_pinterest(num_images=500):\n",
        "    driver.get(\"https://www.pinterest.com/search/pins/?q=vitra%20chair%20design\")\n",
        "    time.sleep(5)  # Wait for page to load\n",
        "    \n",
        "    pin_data = []\n",
        "    scroll_count = 0\n",
        "    max_scrolls = 100\n",
        "    \n",
        "    while len(pin_data) < num_images and scroll_count < max_scrolls:\n",
        "        pin_elements = driver.find_elements(By.CSS_SELECTOR, \"div[data-test-id='pin-with-alt-text']\")\n",
        "        \n",
        "        for pin in pin_elements:\n",
        "            if len(pin_data) >= num_images:\n",
        "                break\n",
        "                \n",
        "            try:\n",
        "                a_tag = pin.find_element(By.TAG_NAME, \"a\")\n",
        "                href = a_tag.get_attribute(\"href\")\n",
        "                \n",
        "                if \"/pin/\" in href:\n",
        "                    pin_id = href.split(\"/pin/\")[1].split(\"/\")[0]\n",
        "                    \n",
        "                    if not any(p[0] == pin_id for p in pin_data):\n",
        "                        alt_text = a_tag.get_attribute(\"aria-label\")\n",
        "                        pin_data.append((pin_id, alt_text))\n",
        "                        print(f\"Found pin: {pin_id} - {alt_text}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting pin data: {e}\")\n",
        "        \n",
        "        if len(pin_data) < num_images:\n",
        "            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
        "            time.sleep(2)\n",
        "            scroll_count += 1\n",
        "    \n",
        "    print(f\"Extracted data for {len(pin_data)} pins\")\n",
        "    \n",
        "    for idx, (pin_id, alt_text) in enumerate(pin_data):\n",
        "        try:\n",
        "            pin_url = f\"https://www.pinterest.com/pin/{pin_id}/\"\n",
        "            driver.get(pin_url)\n",
        "            \n",
        "            # Extract name, description, and image URL\n",
        "            ...\n",
        "            \n",
        "            if img_url:\n",
        "                img_filename = f\"{pin_id}.jpg\"\n",
        "                img_path = os.path.join(image_folder, img_filename)\n",
        "                \n",
        "                response = requests.get(img_url, headers=headers, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    with open(img_path, \"wb\") as f:\n",
        "                        f.write(response.content)\n",
        "                    print(f\"Downloaded image: {img_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing pin {pin_id}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These are images of our scraped chairs and an example of the csv file we have compiled:\n",
        "![scraped chairs](scraped_chairs.png)\n",
        "![csv file example](csv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic Data Generation using Blender\n",
        "We then wrote a script that automates the generation of synthetic chair images using Blender, a powerful 3D rendering tool. It imports 3D chair models (e.g., .obj, .fbx, .blend, .gltf) and renders them from multiple angles and elevations to create a diverse dataset. The script sets up a Blender scene with a three-point lighting system, a camera, and a transparent background for high-quality renders. It also supports material variations, allowing for different textures and colors to be applied to the chair models, further enhancing dataset diversity.\n",
        "\n",
        "The script processes each chair model by normalizing its size, centering it in the scene, and applying textures from a specified directory. It then renders the chair from 12 horizontal angles and 3 elevation angles, producing 36 images per model. GPU-accelerated rendering with Cycles ensures efficient and high-quality output. The resulting images are saved in PNG format with transparency, ready for use in training machine learning models.\n",
        "\n",
        "Here is a code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_scene():\n",
        "    \"\"\"Set up the Blender scene with lighting and camera.\"\"\"\n",
        "    bpy.ops.object.select_all(action='DESELECT')\n",
        "    bpy.ops.object.select_by_type(type='MESH')\n",
        "    bpy.ops.object.select_by_type(type='LIGHT')\n",
        "    bpy.ops.object.select_by_type(type='CAMERA')\n",
        "    bpy.ops.object.delete()\n",
        "\n",
        "    # Add camera and lights\n",
        "    bpy.ops.object.camera_add(location=(0, -3, 1.5), rotation=(math.radians(75), 0, 0))\n",
        "    camera = bpy.context.active_object\n",
        "    bpy.context.scene.camera = camera\n",
        "\n",
        "    # Three-point lighting setup\n",
        "    bpy.ops.object.light_add(type='AREA', radius=3, location=(3, -2, 3))\n",
        "    key_light = bpy.context.active_object\n",
        "    key_light.data.energy = 500\n",
        "\n",
        "    bpy.ops.object.light_add(type='AREA', radius=2, location=(-3, -2, 2))\n",
        "    fill_light = bpy.context.active_object\n",
        "    fill_light.data.energy = 300\n",
        "\n",
        "    bpy.ops.object.light_add(type='AREA', radius=2, location=(0, 3, 2))\n",
        "    rim_light = bpy.context.active_object\n",
        "    rim_light.data.energy = 400\n",
        "\n",
        "    return camera\n",
        "\n",
        "def import_chair_model(filepath):\n",
        "    \"\"\"Import and normalize a chair model.\"\"\"\n",
        "    bpy.ops.object.select_all(action='DESELECT')\n",
        "    bpy.ops.object.select_by_type(type='MESH')\n",
        "    bpy.ops.object.delete()\n",
        "\n",
        "    ext = os.path.splitext(filepath)[1].lower()\n",
        "    if ext == '.fbx':\n",
        "        bpy.ops.import_scene.fbx(filepath=filepath)\n",
        "    elif ext == '.obj':\n",
        "        bpy.ops.import_scene.obj(filepath=filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported format: {ext}\")\n",
        "\n",
        "    chair = bpy.context.active_object\n",
        "    chair.scale = (2, 2, 2)\n",
        "    bpy.ops.object.transform_apply(scale=True)\n",
        "    return chair\n",
        "\n",
        "def render_chair_angles(chair, output_path, num_angles=12):\n",
        "    \"\"\"Render the chair from multiple angles.\"\"\"\n",
        "    camera = bpy.context.scene.camera\n",
        "    bpy.ops.object.empty_add(type='PLAIN_AXES', location=(0, 0, chair.dimensions.z / 2))\n",
        "    target = bpy.context.active_object\n",
        "    constraint = camera.constraints.new(type='TRACK_TO')\n",
        "    constraint.target = target\n",
        "\n",
        "    for angle_idx in range(num_angles):\n",
        "        angle = 2 * math.pi * angle_idx / num_angles\n",
        "        x = 3 * math.sin(angle)\n",
        "        y = 3 * math.cos(angle)\n",
        "        camera.location = (x, y, 1.5)\n",
        "        bpy.context.scene.render.filepath = os.path.join(output_path, f\"angle_{angle_idx:02d}.png\")\n",
        "        bpy.ops.render.render(write_still=True)\n",
        "\n",
        "    bpy.ops.object.select_all(action='DESELECT')\n",
        "    target.select_set(True)\n",
        "    bpy.ops.object.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some images we have generated through this script:\n",
        "![rendered chairs](rendered_chairs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning: Image Post-Processing using OpenCV\n",
        "We perform image post-processing using OpenCV to remove backgrounds from chair images, a crucial step in preparing the dataset for training the diffusion network. The `remove_background_grabcut` function uses the GrabCut algorithm to segment the chair from the background by defining a bounding box. The segmented chair is placed on a white background, ensuring uniformity across the dataset. This preprocessing enhances dataset quality, making it easier for the model to learn chair designs.\n",
        "\n",
        "The `process_folder` function automates background removal for all images in a folder, saving processed images to an output folder. This batch processing ensures efficiency and consistency, critical for training high-quality generative models. By standardizing the dataset, the code improves the model's ability to generate unique and realistic chair designs.\n",
        "\n",
        "Here is a code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARCOcIRNQK70"
      },
      "outputs": [],
      "source": [
        "# Pseudocode for Image Post-Processing Workflow\n",
        "\n",
        "def remove_background_grabcut(image_path, output_path):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Could not load image {image_path}\")\n",
        "        return\n",
        "\n",
        "    # Initialize mask and bounding box\n",
        "    mask = np.zeros(image.shape[:2], np.uint8)\n",
        "    height, width = image.shape[:2]\n",
        "    rect = (50, 50, width - 100, height - 100)  # Bounding box around the chair\n",
        "\n",
        "    # Apply GrabCut algorithm\n",
        "    bgd_model = np.zeros((1, 65), np.float64)\n",
        "    fgd_model = np.zeros((1, 65), np.float64)\n",
        "    cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
        "\n",
        "    # Create a mask for the foreground\n",
        "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
        "\n",
        "    # Apply mask to isolate the chair\n",
        "    result = image * mask2[:, :, np.newaxis]\n",
        "\n",
        "    # Place the chair on a white background\n",
        "    white_background = np.ones_like(image, np.uint8) * 255\n",
        "    final_result = np.where(mask2[:, :, np.newaxis] == 1, result, white_background)\n",
        "\n",
        "    # Save the result\n",
        "    cv2.imwrite(output_path, final_result)\n",
        "\n",
        "def process_folder(input_folder, output_folder):\n",
        "    ...\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            input_path = os.path.join(input_folder, filename)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            remove_background_grabcut(input_path, output_path)\n",
        "            print(f\"Processed {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ...\n",
        "    process_folder(input_folder, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the images we have processed using the code:\n",
        "![cleaned chairs](cleaned_chairs.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A2eqX4ZQwZV"
      },
      "source": [
        "# Model Training: Diffusion Model for Chair Image Generation\n",
        "We then fine-tune a pre-trained diffusion model to generate high-quality chair images from a dataset of approximately 900 examples. Diffusion models work by gradually denoising random noise into coherent images, learning to reverse a process that adds noise to data. We use transfer learning to adapt a pre-trained UNet2DModel from the diffusers library, freezing early layers to retain general visual features while fine-tuning later layers for chair-specific details. The model is trained to predict and remove noise from noisy images, enabling it to generate realistic chair designs.\n",
        "\n",
        "A custom ChairDataset class is implemented to load and preprocess chair images, resizing them to 128Ã—128 pixels and normalizing pixel values. The training loop adds noise to images, trains the model to predict this noise, and periodically generates sample images to monitor progress. A noise scheduler controls the diffusion process, ensuring smooth denoising. After training, the model generates clean, high-quality chair images by iteratively removing noise from random inputs.\n",
        "\n",
        "Here are some code snippets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlJkHc6wQujl"
      },
      "outputs": [],
      "source": [
        "class ChairDataset(Dataset):\n",
        "    def __init__(self, root_dir, image_size=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir) \n",
        "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        return self.transforms(image)\n",
        "\n",
        "def setup_diffusion_model(image_size=128):\n",
        "    model = UNet2DModel.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "    model.config.sample_size = image_size\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"down_blocks.0\" in name or \"down_blocks.1\" in name:\n",
        "            param.requires_grad = False  # Freeze early layers\n",
        "    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "    return model, noise_scheduler\n",
        "\n",
        "def train_diffusion_model(model, noise_scheduler, dataset, num_epochs=30):\n",
        "    train_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "    model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            clean_images = batch\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            timesteps = torch.randint(0, 1000, (clean_images.shape[0],), device=clean_images.device).long()\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "            noise_pred = model(noisy_images, timesteps).sample\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if global_step % 500 == 0:\n",
        "                generate_sample_images(model, noise_scheduler, 4, image_size, f\"samples/step_{global_step}.png\")\n",
        "            global_step += 1\n",
        "\n",
        "def generate_sample_images(model, noise_scheduler, num_images, image_size, output_path):\n",
        "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler).to(device)\n",
        "    images = pipeline(batch_size=num_images, generator=torch.Generator(device=device).manual_seed(42)).images\n",
        "    for i, image in enumerate(images):\n",
        "        image.save(f\"{output_path}_{i}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here are a few examples of the chair images we have generated:\n",
        "![chair examples](chair_examples.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
